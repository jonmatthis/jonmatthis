# Transcript: 2025-08-06-2025 03 17 15 01

## Source Information

- **Source Type:** Local File
- **File Path:** `C:\Users\jonma\syncthing-folders\jon-alienware-pc-synology-nas-sync\videos\video_eater_downloads\playlists\[2025-01-Spring] Neural Control of Real-World Human Movement\2025-08-06-2025 03 17 15 01\2025-08-06-2025 03 17 15 01.mp4`

---

**Total Duration:** 01:30:08

---

## Full Transcript

### Chunk 1 [00:00:01 - 00:09:59]

**[00:00:01]** Okay, hello everybody. And let's get into it. So today we're going to talk about the eye tracking data that we recorded last week. Yeah, last week. Week.

**[00:00:17]** From week ago on Monday I was able to get it processed and sort of like generally visualized and we can sort of work through it. It's not like the cleanest data for a number of reasons, but I think it's a good example of the kind of data you get when you're not super practiced in it. But it's enough to kind of make the main points that we were trying to make about eye movements. It's currently uploading to Google Drive. I'll share this in the server when it's done.

**[00:00:46]** It should take a couple minutes according to it. Yeah. Then we'll start, spend a lot of the rest of the time talking about it and then, yeah, see what we get.

**[00:00:59]** Check in for class stuff.

**[00:01:05]** Here's running slow as hell today. For some reason I'm not sure what I got running on it. I got a lot of running on it, but okay, so yeah, so we are here in week 11 out of 15. Today we're talking about eye tracking data. Next time we're going to do just kind of like a sort of like a gap filling, kind of a lecture of sort of talking about stuff that I feel like I have not focused on.

**[00:01:45]** So one of the main ones I think is I'm going to talk about like neurons as like a specific cell type, which is sort of like a funny thing to talk about in the 11th week of a class on neuroscience. But it's one of those things where like, you know, maybe y' all have encountered like direct instructions on like what a spot a neuron is. You've certainly picked up some like vague understanding of that cell type and sort of how it generally operates. But I think it'll be good and helpful to kind of go in and talk about the specifics down to the level of neurochemistry and synapses and sodium potassium ion pumps and nodes of Ranvier and all that good stuff. And one of the things I kind of like teaching things kind of backwards where you start with the really advanced stuff and you kind of like back your way down to the low level things.

**[00:02:36]** So I think it'll be fun to kind of think about like learn about the specific individual cell, like reductionist, sort of like smallest unit, like the smallest unit of neuroscience would arguably be the neuron. So kind of talking about that after talking about sort of the behavior of these like the large scale Conglomeration of trillions upon trillions of neurons I think would be fun.

**[00:03:04]** Following week on the Monday we're going to spend basically devote the class to kind of doing poster prep stuff and kind of helping you guys do the final formatting and output. And you know, my goal is that by the end of the day everyone has uploaded their PDF to the appropriate spot. So depending on sort of where people are, you can sort of, you can hit different levels of that. It might be, you know, I mean, you have to have it uploaded by Tuesday, which is the day after that class. So you would be wise to set yourself up in a way that you could kind of be done by the end of class on Monday.

**[00:03:40]** And if you're not, then it's kind of up to you. And hopefully even if you're not, you'll have kind of the instructions you need to do it on your own.

**[00:03:50]** And then if it's already kind of done, then we can sort of do preliminary kind of practice talks and kind of just going over the content and the details and just last minute little things. And I guess I'll probably try to leave some time on the following Wednesday to talk about some of the specifics too, just in case you have details that require more than like a class time's worth of attention to clean up. After that I'm going to give talk I like to give about evolution and kind of like the context of, you know, how we got to be this particular strange type of thing. And after that, talk about autonomic nervous system, PTSD and all that kind of fun stuff.

**[00:04:33]** After that is the final class before the actual presentation. So we'll spend that time kind of going, doing more practicing stuff and kind of like going around, you know, small groups kind of presenting your poster to each other so that you're prepared for the following week, which is the poster presentation itself. Monday and Wednesday during normal class time. And then final week of class we'll talk about my dumb bs, which is all the research that I have done in my life, which you will be hypothetically sort of situated to understand the context after everything we've talked about in the class and then last day, last day stuff, wrap up retrospectives kind of looking at. We mostly kind of presenting my personal final project for the course of trying to make sense of all the data from the server and all that good stuff that sound good makes sense tracks roughly with what we've been talking about.

**[00:05:29]** Cool. Okay.

**[00:05:35]** All right, Are we done uploading? Not quite yet.

**[00:05:44]** Okay, there we go. Click on that Share, copy, link.

**[00:06:07]** Come on. General access. Anyone with a link can view it. Copy the link.

**[00:06:21]** Why are we moving so slow today? Computer, where are we here? And links and resources. Eye tracking data. There you go.

**[00:06:40]** It's a 3 gigabyte zip file. So if you want to download it, it'll take a second, but there it is available and let's, let's get into it. So okay, first of all I gotta figure what's slowing down this damn thing.

**[00:07:23]** We should be okay. Okay, so yeah, so if you open up that folder, download and open up that folder, if you so choose, you will find roughly this. So a lot of software that you will use in your life that sort of does scientific recordings and sort of anything kind of like scientific or engineering based. Especially if it's something that hasn't been produced by like a mega corporation that's got the kind of sort of very, very smooth exterior type of thing which you know, folks like Apple and Google like to produce. You'll typically find something that looks like this on the inside where there's going to be just a bunch of like strange looking files dumped into a folder of some kind.

**[00:08:16]** Freemocap has stuff like this. I had kind of like worked to try to make this the top level like recording dump for freemocap. Like somewhat friendly for the brain, but it still winds up looking kind of a lot like this. And so one of the things you want to think about when you're looking at a data dump from a recording apparatus of some kind is like which of these are kind of like the core pieces of data, like what's the primary output of this thing and which of it is kind of secondary ancillary metadata type of thing. In our particular case it is.

**[00:08:54]** What the hell? Oh, I was looking at the wrong one. That's 1024. That's last year. This one is from this year.

**[00:09:07]** There you go, same thing, same idea. Except this one has two eye trackers, two eyes, because the other one does not.

**[00:09:16]** So to that point, this is where we've talked about that concept of a model, a data model, a paper model. All the papers have these same kind of parts, but the content is always different. It's the same kind of thing here where the names of all the things is roughly equivalent but the content is different. So for that you're recording from the sidetracker. The main things I'm looking for are i0i1 and World MP4, which are video files.

**[00:09:49]** Let's see if I click this one. I'm not sure if these play. Yeah. There you go.

**[00:09:54]** So this is the raw video recording from the.

---

### Chunk 2 [00:09:45 - 00:19:44]

**[00:09:45]** MP4, which are video files. Let's see if I click this one. I'm not sure if these play. Yeah, there you go.

**[00:09:55]** So this is the raw video recording from the left eye of the eye tracker. And this constitutes, for the most part, this constitutes the raw data from that particular apparatus. The quality is okay, it looks bad on this, on the projector, it looks better on my screen. But spoiler alert, there is kind of like the shadowy area over here, I think is probably the reason why the data didn't come out as cleanly as it might. It's possible that I could have like moved the camera a little bit, moved it out, because there's a.

**[00:10:31]** There's the illuminator that you can see the reflection of right there. And this area is kind of in a shadow. If you think about like the light coming from this side and kind of like this side of my eye is going to be a bit in shadow. So it's just something. I didn't notice that when I was setting it up.

**[00:10:47]** And I haven't been in the trenches of recording eye tracking data and natural behavior for a while, so I just wasn't quite tuned to look for that type of thing. And even if I had been, I don't think I would have noticed it. But now that I've sort of gone through a recording and sort of seen the data coming out and sort of saying, oh, it's not actually as good as it was. And it's especially kind of like tweaky when my eyes on this side of the screen now I'm clued in to look for something different whenever I'm going back and recording more data. So it's that kind of thing, that sort of like, iterative aspect, that is something I think really important to pull out of this kind of this little demo, because that is the way that you ever get good at anything.

**[00:11:34]** Like, if you're going to try to use some complicated piece of equipment, do some kind of complicated task, the only way that any human has ever gotten good at anything is to do that thing over and over and over and over again. And eventually you kind of. You learn enough of the things to look for and you sort of learn them like, sufficient a number of times in a row that when you go to do the recording, you sort of know to look for all the little bits and pieces and you know, and you learn that looking for clear, even luminance across the image is an important thing that I need to look for. And if you, with enough time, that's where expertise comes from. I remember, sure I've said this before in this class, but it's worth repeating.

**[00:12:20]** I remember being in the undergraduate graduate student era of my life and then just seeing people doing stuff and just being baffled at how do you learn all of that? How do you know how to do all these things? How do you know every paper that's ever been published? And how do you know the names of all these weird parts? And how do you know when I say, hey, yeah, I'm ready to go, this looks good.

**[00:12:43]** And you come over and say, oh no, because of this, that and the other thing, it's not look good. And the answer is only that they were like 20 to 30 years older than me and they had been doing the thing that I was focusing on for as long as I had been alive. And over time they had made enough mistakes that the obvious stuff becomes obvious and they're able to point it out and it doesn't wind up now that I'm on roughly closer to that side of the fence than the other. I'll say it doesn't. Expertise never really feels like you're an expert.

**[00:13:15]** You sort of just notice that there's a lot of people that are worse at things than you are and then you kind of just how to make good with that. Typically also by the time you're sort of at my era and above, you've specialized enough that the domain of things that you have to think about is narrower. So that when people talk to you and like, holy shit, you're got this high level skill set, it's like, yeah, but it's like this wide. And then when you're a student, you're typically bouncing from room to room talking with experts in different disparate fields, experts talking about their expertise in an environment where they only have to care about their expertise. It's pretty easy to imagine that you can get a skewed perspective of the world when you're sort of living that kind of life.

**[00:14:03]** But anyways, long story short, that's an eyeball. It's doing a good enough job. And the signal that we're going to try to extract from this raw piece of data is roughly speaking, the position of this black patch as it moves back and forth up and down throughout the image.

**[00:14:31]** So let's do this kind of funny sitting here talking about expertise with eye tracking data that's not even all that clean.

**[00:14:46]** So I think let's do the similar kind of like tracing path that we've done before around measurements and data and Recording and data collection and all that good stuff, because we are now. So this is very similar to the motion capture data in a lot of ways. It is a video record that is recording some aspect of my motor control and recording the parts of that neural cascade that affect the world enough to be noticeable by a camera. And now we're going to try to sort of process from that raw data down through various levels of abstraction and computation and sort of like mutation of the data into a format that we think we should be able to use to make claims about not only what was going on inside of my oculomotor system and visual cortex and all that kind of stuff, but hypothetically even have things to say about all human and mammalian and primate visual cortices and oculomotor systems, which is a pretty baffling thing to do from the height of our hubris, but came all the way here, so we might as well start.

**[00:16:16]** Yeah, so similar to the previous data, you start with this image.

**[00:16:30]** So you have a camera and it looks at an eyeball and light, actually in the previous case we were talking about light coming from the environment. And then I did a little picture of a sun and the light comes here and then bounces off that into the camera. In this particular case, because this is an infrared camera and there really isn't very much infrared in the room, we actually rely on a secondary illuminator that we attach to the camera. And so the light in the infrared spectrum, like 800 to 1,000 nanometers or whatever, bounces off the eyeball and into the sensor. And the sensor produces a bunch of images of eyeballs.

**[00:17:26]** In this case, for the Icameras, I think it was recording at about 120 frames per second. So with the motion capture camera and then with the world camera and with many cameras like this one, a standard default frame rate for most cameras that you would pick up is about 30 frames per second. Like some higher quality cameras like to record at 60 FPS. You know, you don't tend to see a lot of, like, video media for standard human consumption recorded at higher than that, higher frame rates than that, unless it's being billed as, like, slow motion. So if your phone has a slow motion mode, that slow motion mode will be recording at a higher frame per second, but then it will be played back at 30fps.

**[00:18:07]** So you can, if you record 240 frames per second for one second, then you play that back at 30 frames per second, then that is what, at 1:8, the frame rate. Another of My many gripes about how we handle things like cameras is if you do have that mode in your phone, it'll probably say things like 18 speed or 1/4 speed, assuming that the default frame rate is 30 frames per second. But they don't actually tell you that because your phone wants you stupid and because the more you understand technology, the less you're reliant on the people who are selling you technology, selling it to you in exchange for your data, and pretending like it's a even deal.

**[00:18:52]** But yeah, but in the end, like with the way that we tend to analyze computer vision and images from cameras, we generally think of a video as just a stack of frames that sort of comes in at a particular rate over time. And so then we try to extract data from each frame. A little data blob here.

**[00:19:21]** And then sort of, you know, we have like frame one, frame two, all the way up to frame N, which is however long the recording is and the time. So, so if the data from one point to another, we want to think about that in terms of the time it takes. We have to have information about the.

---

### Chunk 3 [00:19:30 - 00:29:29]

**[00:19:30]** Is and the time. So if the data from one point to another, we want to think about that in terms of the time it takes. We have to have information about the time interval between the frames. And if we wanted to do that sort of in a back of the envelope, kind of like sloppy good enough for visualization purposes, ways we can take what we already know about the frame rates of the cameras, or sort of take the fact that I've already looked at this data and seen that the mean frame rate is around 120fps and sort of use that information. But if you want to be more careful about it, we can look back to this base data and notice that we have, for each video, we have, among other things, a file called i0timestamps.

**[00:20:24]** I1 has i1 timestamps, and the world one has world timestamps. So when I talk about the raw data from this type of a system, generally the base raw data, like the data that I really, really care about, is the video files. The biggest blobs of data in terms of how much data is in this folder you can see that the, the videos are somewhere around half a gigabyte to 1.3 gigabytes, and the other files are like in the kilobytes and below sort of data scale.

**[00:21:03]** But the timestamp data is also, technically speaking, just as necessary to make proper empirical sense of these recordings as the actual videos. Because videos generally, and I think by generally, I think almost universally, videos do not actually encode real measured timestamps from their frame rates. If you ever find a camera that produces something that looks like a timestamp, like GoPros will do, stuff like that, they're lying. They're just taking the number of frames and dividing it by the duration of the recording and then splitting those up across all the frames that they actually get. So if you look at the variation between the frame durations from something like a GoPro output, they'll be exactly the same.

**[00:21:56]** Every frame will take 0.0333, 0.00, whatever, 33 milliseconds, because it's not an actual measurement. If you were to look at these timestamps, you'll see some noise in the data because it's an actual data point that has variation. But that's really more than we need to be going into at this juncture.

**[00:22:30]** The important thing is that for these cameras, you're pulling out data on a roughly frame by frame basis. I mean, in this particular case, it's fully frame by frame basis. But in other Contexts of camera stuff. You might have some aspect of using the previous frame to smooth out the data or clean things up, but in this case, we're going to do it sort of frame by frame. Yeah.

**[00:22:59]** So would that not generate.

**[00:23:06]** Yeah. Which part? I was thinking about how, like, in the past, like, physics projects, I've, like, had to use like a small camera. Yeah, you were, you were. Go ahead.

**[00:23:20]** Yeah, so. Yeah, okay, I get it. Yeah. Yeah. So like, so, because the reality is, is that like, for something like a physics class and sort of like, you take the camera and you're sort of plotting it over here and you're saying, okay, like these are the positions at frame 1, 2, 3, 4, and it's A.

**[00:23:37]** Let's say it's 100 frames per second camera. To make the math easy. Then we say, okay, the time elapsed between this is 10 milliseconds. And so if we go from whatever, like 1 to 3 over 10 milliseconds, then we're meters going very fast. So then we're traveling 3 meters in 10 milliseconds.

**[00:23:57]** And that's where we get the all. All the physics y stuff comes from there. This number was not as precise as you may have thought it was, which for your physics class, totally fine in reality. Because the reality too is most cameras actually do work pretty well. So the variation between frames probably wasn't that big.

**[00:24:21]** So let's say maybe it was from 9 to 11 milliseconds per frame and just on the average it winds up being 10. Then if you were to calculate how much kinetic energy is occurring on frame one by frame two, your estimate's going to be a little bit off on a short enough time scale, but over the course of 1000 frames, it's probably going to wash out. But you get into these questions of precision. If you're like, what was the application in that case? Were you trying to land something on the moon or were you trying to sort of say, oh, yeah, look like there's a roughly ballistic trajectory and the kinetic and potential energies trade off or something like that.

**[00:25:09]** And so, like, for a given application, it's a very good and valid point that you have to be. You should be mindful of the. Of the reason why you're taking the recording when you're asking the question about how much precision really matters. If you're up to the point where you're designing passenger jets or something like that, it might be worth getting the real numbers, but in most context it wouldn't super matter. And it's Also, if, if the actual rate is 9 to 11 milliseconds per frame, if you're looking at the specific physics of a singular frame, then that could be off by, that's a factor of what, 10 to 20%.

**[00:26:00]** But if you have 10,000, 100,000 frames of data, then it'll kind of wash out. So it really kind of. Yeah, it just depends on the application. A lot of things with the motion capturing stuff too, it's like I am building the free mocap tool for clinical applications where we want to maximize precision. But for a lot of the history of the project, a lot of the stuff that I produce out of it is just taking the estimation of the mean frame rate as the assumption for the physics because it's enough to make the point and it's a complex enough system that I just don't have.

**[00:26:41]** It's not the highest priority thing to sort of make sure that that lines up at least at the way that we've been using it so far.

**[00:26:49]** So yeah, and I think what is, and what's sort of a common theme in things like neuroscience? Neuroscience in particular, I think, but a lot of science in general, particularly on this kind of like empirical tool building sides of science, you want to take every, you want to sort of turn down as much of the noise as you can in your data because it is, this is the base data, right? This is the, like you're gonna, this, we're taking these measurements and we're eventually after a while gonna be trying to say things about like how the brain works. Which again just to be clear, that's a lot. That's a big thing.

**[00:27:29]** That's a very bold thing to believe that you can do is to look at a stream of sort of blurry images of the eye and say things about the brain. So the last thing you want to do is just leave a bunch of noise in the system that you could get out of it because that noise is going to hinder your ability to look at that type of thing, to look at that, to get information, to make the kind of claims you want to be able to make. And like. Yeah, so for most kind of behavioral settings, plus or minus a millimeter plus millisecond isn't really going to change much of the output. But I'm working with people building similar types of systems that will be coupled with electrophysiological spikes from the brain.

**[00:28:12]** And that's a system where you kind of do want things to be nailed down to the sub millisecond level. If you can so it's all just kind of a matter of in a perfect world we smush as much of the error, we smush all the error down to zero and we're sort of like we're having perfect empirical measurements of true reality. In the unfortunates of real world we sort of do what we can with what we've got. And being a good scientist sort of means understanding the system that you're studying and the context in which you're studying it enough to know when a certain amount of data is like what is permissible and impermissible sort of levels of noise in your data set.

**[00:29:01]** So in this case, if this was data that I was going to try to build a research project off of, I would throw it in the garbage and say let's do it again.

**[00:29:11]** Not because it is garbage but because I could do better. So I'd say ok, this is pilot data, good try, let's try it again and get the cleaner stuff out of it. In this particular context it's good enough for the purposes of this course. And like I said before, I kind of like it when it's a little noisy because.

---

### Chunk 4 [00:29:15 - 00:39:15]

**[00:29:15]** And so I'd say, ok, this is pilot data. Good try. Let's try it again and get the cleaner stuff out of it. In this particular context, it's good enough for the purposes of this course. And like I said before, I kind of like it when it's a little noisy because then we get to have conversations like this, because this is closer to what your reality will be when you start collecting your own real data from weird equipment than the world where you push the button and the light goes green and everything is happy and good.

**[00:29:44]** And I thought about delaying and sort of like, you know, pushing off, but then I'd have to, like, use data if we didn't record in class and, like, drop a lecture. And I don't want to drop any more of those lectures if I can. So, yeah, so here we are. Good question, though. Any other empirical anxieties to share about unknown error in previous class assignments?

**[00:30:10]** This is also one of those things where, like, there's a very, very small percentage of people in the world who give enough of a shit about the timestamps between camera frames to, like, talk about it in a classroom setting in this kind of context. But, like, I have spent a lot of my life pulling data out of video streams. And so, like, the realization that most cameras will produce, like, fake timestamps, that was like three to six months of my postdoctoral experience was discovering that was the case. So now you have it for your own life.

**[00:30:47]** Okay, so base data are these images.

**[00:30:59]** And this picture is actually. So the videos from the mocap stuff was a little more complex because it was a color image. So not that much more complex. But it is like, you know, but this, this is a simpler, simpler beast where it is a. Specifically, I believe, I believe it's either 192 or 400.

**[00:31:24]** I'm not sure. Let me check. I think it's 400.

**[00:31:31]** Yeah.

**[00:31:37]** So it's a 400 by 400 grid of pixels.

**[00:31:46]** And each of these pixels is going to have a value between 0 and 1, where 0 means black. Like, there's no, no data. Like the, you know, the camera sensor here is basically going to have some physical silicon wafer that is, you know, designed that when light hits it, it changes its voltage output. If you're seeing an analogy between this and the back of the retina, guess it is. It is there, but only analogously in the sense that I drew like a wiggly thing coming in and a change in voltage going out beyond, well, going through a lens and bending and all that kind of stuff.

**[00:32:33]** Beyond that analogy, it really breaks down. Like it's not. Like this is a very, very different thing than the thing on the back of your retina or I guess the retina. But in terms of their base level machinery, they are both things that turn photons of light into voltage output in a way that retains some of the structural information about the way that the light came in. So take from that what you will.

**[00:33:03]** But yeah, so this, the image that comes out is going to be somewhere between white. So this is an example of white. And this pixel basically is. So the pixels, the numbers are coming in between 0 and 255. 255 is 2 to the 8th minus 1, I guess.

**[00:33:26]** But so this pixel, if we were to go in there and measure it actually. Can I do that?

**[00:33:37]** I cannot. The value coming off of this is probably pretty close to 255. Like that's fully saturated. If anyone ever does photography. That kind of like that zebra stripes thing, that it will show up if the image is fully washed out is telling you this is the place where the sensor is maxed out.

**[00:33:58]** We cannot measure any variation from here to here because every pixel in this region is going to be 255. Okay, that's fine though. We're okay with that. This pixel here is probably. This is an example of a black pixel.

**[00:34:21]** And this is probably producing something pretty close to zero and sort of across this region here, all of these pixels are probably pretty close to zero.

**[00:34:30]** And that's fine. And then all these regions here, those are going to be some other number. This is zero, this is one. This is going to be some other number between zero and one. And so this whole grid is going to be filled with numbers, some of which will be 0, some of which will be 1, some of which will be 0.5, whatever.

**[00:34:53]** And that corresponds. That is the raw data that we have extracted. It's the voltages coming off of the grid on the camera converted into a number between 0 and 1.

**[00:35:10]** Somewhere in the settings there's changes for things like gain and exposure and stuff like that, which changes the physical mapping here. But in any case, this is the base data that we have.

**[00:35:25]** Probably wish I might regret erasing that. In fact, I'm pretty sure I will.

**[00:35:41]** So this already despite being 400 by 400. So 400 by 400 is whatever, 16 with four zeros, is that right? 160,000 pixels times 120 frames per second is six times. Yeah, 16. 160,000 times 120 FPS corresponds to whatever that number is, bits per second.

**[00:36:21]** So this is a number between 0 and 255, which is 0 to 2 to the 8th. And this is a byte. So I think. So I think that's a byte. I can't remember.

**[00:36:40]** But yeah. So when you talk about bits and bytes, like bit is 0 or 1 byte is usually that many. And so 160,000 of those times 120 FPS is how many bytes per second is coming off of this. Which is to make the point that it's a lot, a lot of data. But also relative to the true facts of the universe, it's nothing.

**[00:37:01]** It's absolutely nothing. It's a pale, vague shadow of reality. If this was a 4K image recording at 1000 frames per second, I would still say that that's a dim shadow of reality. We would be able to say far more about the boundaries between the pupil and the iris and the sclera and the eyelid in that context. But it would still be the case that if we doubled all of those numbers, it would still be a pale shadow of reality.

**[00:37:39]** But once again, as we've discussed, whether or not it's a usable pale shadow of reality is dependent on the applications that you're trying to do.

**[00:37:51]** So anyways, feel like that point has been effectively belabored.

**[00:38:08]** So we get these Images, we get one per this one E. Yeah, one frame. So 120 frames per second is 8.33 repeating milliseconds per frame. So every eight and change milliseconds we get a new one of these dang things.

**[00:38:33]** And so we're trying to make sense out of what we can pull out of it.

**[00:38:54]** So think about what type of data we would want to pull out of a given frame.

**[00:39:06]** So actually, and this is a. Actually not quite yet, but soon we'll have basically a division point.

---

### Chunk 5 [00:39:00 - 00:49:00]

**[00:39:00]** A given frame.

**[00:39:06]** So actually, and this is a. Actually not quite yet, but soon we'll have basically a division point in the hypothetical processing algorithm that we can use for this thing based off of the science. Based off which part of the scientific inquiry you're trying to pursue. But base level, what we want out of this is the position of the pupil. Like, we want to know where the pupil is really, we want to know where the pupil is relative to the head.

**[00:39:39]** But we don't actually. This data does not provide any information about the position of the head. But we know, because we were here that this view is coming from a camera that was placed here and we can say didn't move relative to the head as I moved my head around.

**[00:39:57]** So there you go, I got ears and everything.

**[00:40:20]** So, yeah, so this starts getting into some of the levels of geometry which are really not necessary and relevant here. But we know that this camera, the sensor that produced this data is located in a fixed position relative to the head. So the data that it gets is in sort of camera coordinates. Like this is in the camera coordinates of the eye. Camera image coordinates are upside down.

**[00:40:56]** They start 0, 0 is the upper left corner of the screen and then you kind of count that way. Actually don't go back, you just go that way.

**[00:41:09]** So positive Y points down, positive X goes that way. It's one of those things that like, it makes, it doesn't. If you look at it from the perspective of like, you know, like whatever, whatever layer level of math, they start teaching you how to plot stuff on an XY coordinate system, it's confusing because in this context, positive Y, positive X goes this way and positive Y, positive Y goes up. But in image coordinates, positive Y goes down. So if this is 0, this is 100, this is 200, this is 400.

**[00:41:47]** So down is up. When the number goes up, the pixel goes down.

**[00:41:54]** So you get different reference frames in different places. So zero indexing is the big. That's mainly encoding. You start counting from zero. So it's like you do zero, 1, 2, 3, 4.

**[00:42:06]** So it's like this is four instead of. It's confusing. But this is. The index is 4, but the number is 5. And you often talk about what coordinate system are you using.

**[00:42:20]** So image coordinates, you start on the upper left and go down and go right.

**[00:42:27]** In that case Z, like if you have a Z vector, if. So if this is if it's the right hand rule. So X, Y, Z. If you're doing that correctly and you put that number there, positive Z is that way sometimes you'll see stuff where it's like negative Z is that way because they're doing a left handed coordinate system because no one told them that they shouldn't. But yeah, and you get so like.

**[00:42:56]** So for example, like when I go back and forth between like my biomechanics sort of robotics people and then go over to my like vision sort of neuroscience people in biomechanics and robotics and stuff like that, the X, Y plane is almost universally the ground plane because if you're worried about physics, that's the first thing you want to know about. So you have, you put, you start counting with X, then Y, then Z and you sort of put the most important, sort of the first two elements on the most important plane, which is the ground. And so in that world Z points up. But then you go talk to like people who do like animation or do virtual reality types of research, just thinking about vision and think about like the image plane. And then for them XY is the image plane and Z points back.

**[00:43:49]** And it's the kind of thing where if you have lived your whole life studying vision and then someone comes along and says Z points up, people will have sometimes very strong reactions to that being like that's wrong and incorrect because they have not heard about different fields that use different things. And it's a very, again, it's part of the very cultural aspects of science where it's very easy to live in a particular intellectual tradition where X, Y and Z always correspond to the same thing. It's like if someone started making a plot in a class where they say, okay, this arrow, that's Y and then this one here, that's X. That would feel weird, but it's also completely arbitrary. As long as you're doing, then that would make this direction Z.

**[00:44:42]** It's arbitrary. X and Y. Don't like they could be anything. We just chose those because they're like the weird letters at the end of the Alphabet. I'm not quite sure where we got that from, but yeah.

**[00:44:56]** Whether or not the data is one way or the other, the numbers don't really care. The measurement doesn't really care if you're measuring it in one reference frame or another. So everything after this point kind of becomes points of convention. Like the data is produced in a certain way and it's recorded in a certain way. So that when humans encounter it, they will know if they, if they're like sort of indoctrinated into the appropriate intellectual traditions, the numbers will make sense to them and if they sort of make guesses about things, it'll be sort of roughly correct.

**[00:45:29]** But it's a very. Yeah, this is where some of the, like. Yeah, I don't know. It's. It's really easy to sort of fall into a position where it feels like there are such things as right and wrong answers, and there are right and wrong answers, but almost universally it's only right and wrong relative to some cultural norm or another or some tradition or intellectual tradition or another.

**[00:45:57]** Or like the Internet runs entirely on the basis of protocols. Like they have HTTP, which is the hypertext transfer protocol. You have email, which is like a different kind of protocol. And for that you're slinging just data blobs back and forth. And when your computer receives it, if you go to a website and it gives you something which is not structured according to the HTTP protocol, your computer will reject it, saying, this is an incorrect website, invalid site, or whatever.

**[00:46:21]** But there's nothing explicitly wrong with that data. It just doesn't follow the conventions that we have all agreed that we're going to follow. And so if you're a computer, you say, I don't even want to deal with your bullshit. Just, you know, send me something correct or I'm just not going to show this webpage. Yeah.

**[00:46:36]** The important thing, the point that I really, really want to belabor as much as possible is like, there is a very, very strong cultural element to any form of scientific exploration. Yeah. Kind of unrelated, but it kind of made me think of it. Do you know, in like image compressors, how does that. How does that work?

**[00:46:55]** Yeah. So that is a side note, but I will go through it because I can do it quickly. You happen to have said, if I didn't know about that, I would say, I don't know, ask the bot. But the way that they do it is basically by taking advantage of the things that I was pointing out. So when this data lives in numpy on my computer, which you don't know if that is.

**[00:47:21]** But don't worry about it. Like when it's stored raw, like what I've been describing here is how to store that image raw. And it's saying, I'm going to record every single number from every single bin, from every single thing. And if you do that, the numbers get really big, really fast. Like, if you were to like, I think this literally is bytes.

**[00:47:38]** So if you wanted to like 16. What is that? Like, it's like 160 kilobytes per frame. So kilo. That many mega.

**[00:47:49]** I don't know kilo, no one knows.

**[00:47:55]** And that's the raw uncompressed form of that. However, as we pointed out, all of these are 255, so I don't actually have to record every single dot here. If I have a way of saying everything in this particular box is 255 and so now I can replace. This is called a bitmap. If you ever encounter bmp, it's a bitmap.

**[00:48:22]** It's just every dot in a grid. If you encounter something called a. JPG or a. Png, it has replaced. Basically it just makes there's some decision of which numbers are we going to consider to be the same number and it looks for blobs where it can basically replace. Let's say that this let's say that this region right here is 10 by 30 pixels. That's 300 numbers you have to record.

**[00:48:53]** So if you can somehow find a way to record something that says everything in this region is this.

---

### Chunk 6 [00:48:46 - 00:58:44]

**[00:48:46]** Say that this region right here is 10 by 30 pixels. That's 300 numbers you have to record. So if you can somehow find a way to record something that says everything in this region is 255 and you can record that in less than 300 numbers, then you have now achieved compression by doing that. And with something like a jpeg, it's a lossy compression, which means that if you compress it and pull it back out, you lose data like information. You cannot reconstruct the full thing.

**[00:49:20]** If you have something like a png, that is a lossless compression. So you can compress it down and then you can uncompress it and get the exact same image back, which is obviously advantageous. But then it takes longer to do and it doesn't compress as much. So it's again, kind of like in the deep, deep guts of Free mocap, I make decisions about like, oh, I wish I could compress this losslessly, but then it takes longer to do. So I have to do a lossy compression, but then I have to set the parameter of what number is considered to be the same number.

**[00:49:56]** Because it's worth it to lose the data to gain the time to do the. So it's that basic idea for sort of like you find ways of saying, like, oh, we don't actually have to record this in this super inefficient way, but there's more efficient ways of doing it.

**[00:50:16]** Yes, I will allow that rabbit hole because it is dear to my heart.

**[00:50:24]** Okay, so.

**[00:50:32]** So if you recall, when we're looking at the motion capture data, I talked about the magic step, there's a magic box in that equation called a convolutional neural network, which is a machine learning trained neural network whose job it is to identify human shapes in images and draw stick figure skeletons on top of them. And I call it a magic box because it is an inferential. It's an inferential equation that involves trained data and training sets. It's neural networks, it's machine learning. Technically, it falls under the band, the sort of the umbrella term of artificial intelligence.

**[00:51:15]** Because AI is an imprecise term that is sort of. AI is an imprecise way of saying machine learning.

**[00:51:23]** In this particular case, for this type of eye tracker, there is no equivalent magic box. This is all done. This tracking is done with old school computer vision where I don't know all the algorithms, but I've looked at them. I couldn't do the math by hand. But every step of the process to analyze this is computational in nature.

**[00:51:50]** There is no statistical trained neural network. There is no, no one's going in and like labeling, hand labeling the images. Every step that's done is done on the basis of looking at things like the gradients between light and dark and light and dark and stuff like that. Like if you were to take a slice through this image and just grab the luminance of the pixel from left to right, it'll look like, so it's bright and then it drops down there. And it drops down there.

**[00:52:22]** And it drops down there, drops down there. And so this we know because of our giant human brains and our sort of well evolved visual system that these pixels here are sclera, these pixels here are iris, and these pixels here are pupil. And this is the luminance between one, let's say actually one and let's say zero down here. So you see, it's a little bit brighter up here. Then it goes to kind of like a shade of gray and then it goes as dark as it can get and then back out.

**[00:53:03]** And you can do this in that sort of one dimensional slice. You can also do the same thing from top to bottom. And if you dug through the data, you wouldn't find anything that does exactly this. But I'll just say that the way that this algorithm does its pupil detection relies on that kind of like low level sort of analysis of the raw pixels. And there is no computational, there's no inferential step, it's all just computation on the raw image.

**[00:53:33]** This is roughly speaking, what I call the difference between what I call classical computer vision and contemporary computer vision, where basically the whole tech industry has just decided to forget how to do geometry and just put all their eggs in the neural network basket. But that's okay, I understand why. But in any case, at the end of all that process, there's a bunch of chunky gross data and the output of that is an ellipse that is drawn around all the darkest pixels in the image.

**[00:54:18]** And that ellipse is sort of assumed. This is a, it's called a dark pupil tracker because it tracks the dark pupils and it sort of gets the, an ellipse there and say, okay, we're estimating that there's your pupil is there. And in the sort of the data frame of the camera, we can take the average through the center position of that ellipse and then we get that is your pupil X and that is your pupil. Wait, no, that's your pupil Y and that's your pupil X.

**[00:54:53]** So if you're me. And you care about the whole question of where are people looking at given points in the image when they're doing this, that and the other type of activity. This is the data that you want. You want to know the position of the pupil in the frame.

**[00:55:13]** If you're a pupillometry person, if you believe that you need to be studying the size of the pupil and you want to look at that, that data stream, one, which I have previously in this class and in the future and for the rest of my life, sort of talked a fair amount of shit about the peopleometry side of the world. Not that there's no good information there, but it's just overemphasized by people that don't want to do real calibration.

**[00:55:37]** But if you are one of those people, then you care about the size of this ellipse.

**[00:55:47]** Because if you're looking at information about pupil constriction, the size of this ellipse is a very important data stream for you. So you might be willing to actually throw away this information of the X and Y because you don't really care where they're looking. You only want the pupil diameter. Whereas if you're me, I am more than happy to throw away information about the pupil diameter and only get out X and Y. In this particular case, it saves out both.

**[00:56:14]** But if I was writing the code from scratch, I might not even actually, I probably would get and record the pupil diameter because you kind of get it along the way. But if I had to choose, if I was trying to save on processing time somehow, I would happily throw away the diameter, not the other one. Which sort of. That was that bifurcation point I mentioned, where depending on what type of science you're trying to do, even this close to the raw data is you get this sort of splitting off of paths.

**[00:56:45]** So this gives me pupil position over time. Xy. And remember something that I said about eye tracking, which is not represented in this data? If you look at the.

**[00:57:01]** It's one of those, like, trick leading questions, which I always hate. Why aren't we playing? Hey, buddy.

**[00:57:11]** Oh, is that going to come in upside down?

**[00:57:36]** Come on, man.

**[00:57:43]** There we go.

**[00:57:47]** So it might be hard to see, actually, but I talked about it a little bit. So if I'm only recording the vertical and horizontal position of the eye, am I getting everything that there is to know about my eye position on each frame, or is there something that I'm missing?

**[00:58:13]** So it's like a sphere. So if I'm looking at the up, down, left, right. Is there anything else that I'm missing? Well, so. And it's not going in and out.

**[00:58:21]** So it's a sphere that's fixed in space. So the distance is there. But. What? So.

**[00:58:30]** So it's. And it's attached to my head.

**[00:58:34]** What's that rotation? The torsion. Yeah. So it's this. This axis.

**[00:58:40]** So it is. It is. So this is actually. What's that?

---

### Chunk 7 [00:58:30 - 01:08:30]

**[00:58:30]** So it's attached to my head.

**[00:58:34]** What's that? Rotation. The torsion. Yeah. So it's this axis.

**[00:58:40]** So it is. So this is actually. What's that? I guess so it would be. Oh yeah.

**[00:58:47]** So it's in terms of rotation. Because these are now spherical coordinates. They haven't talked about, but it's worth talking about. So we've been talking mostly about like. So this position here is X.

**[00:59:04]** So X, Y. Right. It could also be L. Theta. Right. You define it.

**[00:59:14]** And that's polar coordinates versus Cartesian coordinates. In 3D space you have X. Let's just say Y and Z and you have a point that's sort of like out here along all three axes. Similarly, you can define that in terms of a distance and then theta and then like whatever, rho. A third.

**[00:59:44]** So there's still a three dimensional. What's that? Rho would be distance. Rho would be distance. Yeah.

**[00:59:49]** So and this is a case where like. Because if we assume that it's a sphere, if we assume that it's a sphere, that doesn't change radius, then it's actually I would. In most contexts I would say like these things are equivalent and they are absolutely equivalent. But if I'm measuring stuff in. Now forget that it's an eye tracker, but let's say I'm measuring stuff in 2D and I know that the data is only going to be spinning around in a circle and that this circle is a, is a fixed distance.

**[01:00:19]** I would much rather convert that into polar coordinates because then I can throw away this L. I don't need that. So now instead of having to worry about two numbers, I only have to worry about one number and it's that theta. So similarly for this, if we assume that it's a sphere in space, to know the position at the tip of that, of that vector, technically I have to have three numbers in terms of the theta. So it's like azimuth and elevation. So elevation makes sense.

**[01:00:48]** It's up and down. Azimuth is like rotating. Like if I'm pointing at something like this is elevation and then this is azimuth. But there is. So I would much rather do that because then I only need theta in rho and I don't need that Z axis.

**[01:01:04]** But there is a third variable that I technically do need and it's the rotation around that point. So it's still. It's kind of. That's why it's like, it's like, oh, it's a zda. It's like, well, it's kind of like it's in that space of like two dimensional number, like two dimensional number, three dimensional numbers, which if that blows your mind, just, you know, that's like matrix math, linear algebra.

**[01:01:29]** Long story short about linear algebra, sometimes numbers can be grids of numbers and they act about the same. So that's a two dimensional number.

**[01:01:39]** If you have a bunch of them, that's a three dimensional number.

**[01:01:43]** I think linear algebra is probably one of the fields that we teach the worst because it's the most useful kind of math that is taught in the least interesting way. So good luck if you have to take that.

**[01:02:01]** But yeah. So torsion is not measured by this eye tracker or any eye tracker that I am aware of because this pupil, this dark pupil algorithm has an unconstrained degree of freedom. It does not like the way that I described measuring this and sort of looking for the darkest blob in the scene. There's nothing in there that tells me anything about rotation around that optical axis. So this is one of those things where it might be hard to see here, but I do it explicitly.

**[01:02:38]** See here I'm moving kind of back and forth and you can tell too.

**[01:02:50]** So if I'm here.

**[01:03:09]** Oh, I guess I stopped doing that.

**[01:03:20]** So there's, there's the eye.

**[01:03:24]** Oh, I don't want to do that.

**[01:03:36]** I wish I had a better way of viewing that. My videos, I need a better way to do that anyways.

**[01:03:43]** Having a hard time like identifying that thing, but you can see anyways. I'll leave this one as an exercise to the reader. This is one of those things that if you look at the. Look at. This is the game of look at the video.

**[01:04:04]** Take a video on your phone, look straight into the camera and then rotate your head like this. You'll see your eye doing torsion. It'll do like plus or minus 7 degrees. And it's like if you click, you'll see it kind of like if you rotate your head like this, it'll go tick, tick because it'll go to the extent of its abilities and then tick back to zero. And that's ocular torsion.

**[01:04:28]** And it's one of those areas of visual neuroscience that like, if you read the literature of, at least the literature of like 10 years ago, you'll be able to find people saying we don't need to worry about torsion. Isn't it nice how we don't need to worry about torsion and they're just straight wrong. That's just the place that the science is Wrong. And it's because there has. Because the field has for so long been looking at people only head fixed where they don't rotate their head so you don't see torsion.

**[01:05:02]** And because we've been using tools which don't measure torsion, the culture has sort of like extracted this belief that you don't need to measure torsion. And for just things that they tend to study. They're true. That's true. But if you want to know.

**[01:05:16]** So let's talk in terms of like desiderata of like the things that I really, really want to get out of this data to really understand the nervous system. I want to know if this is the eyeball and it's attached. That's not how that works at all. Jesus. Would it be like.

**[01:05:36]** That's also not how that works. Okay, See, it's the other way around.

**[01:05:47]** There we go. Yeah. Took a second. Oh, one more time. Good.

**[01:05:55]** Got to get it right. Very important. There will be a test for me only not for you. Okay, there you go. That's good enough.

**[01:06:02]** So that's your brain, that's your eye. Visual cortex does its thing. Goes back here, you got a retina and everything. And then the light from the world is coming in. I want to know if you can give me an eye tracker.

**[01:06:16]** Like with the eye tracker that I have, which is the best one I'm aware of.

**[01:06:22]** I want to be able to have a measurement of where if there's things in the world, this is a tree and this is a cat.

**[01:06:35]** The light from these objects, as it hits the eye, I want to know where on the retina that light is being recorded. Remember my desires with this type of a tool is to have an insight into the nervous system, into like the visual cortex, the oculomotor cortex. So I want to know enough about the eye position to be able to make the estimates about where. Like the geometric projections of objects in the world, you know, map onto like parts of the retina. So the main thing that I need for that is the horizontal and vertical position, because those are the big movements there.

**[01:07:11]** But if I'm. If I'm ignoring the fact that the eye is rotating, then I'm going to get that answer a little bit wrong. And it's a very similar type of thing to what you were talking about with the timestamps. It's a little bit wrong. I can do way, way more with an eye tracker that records horizontal and vertical position, it ignores torsion, than I could do with an eye tracker that records horizontal position and torsion, but not vertical position, which is probably why that type of an eye tracker never would never exist, independently of the fact that it's just like, it's hard to measure torsion.

**[01:07:46]** Like, it's a more difficult problem.

**[01:07:53]** There are other ways of. There was an era where we did one of the ways of measuring eye track, I think still. Well, I think probably still the most accurate form of eye tracker are magnetic coils. So. So you can put, basically, contact lenses that have copper coils in them, and it's not like regular contact lenses.

**[01:08:16]** There's a little suction pump so that they stay fixed, and then you put the head in a big kind of magnetic field, and you can measure the eye movements using that with a very, very high level of accuracy. And I think those probably do give you.

---

### Chunk 8 [01:08:15 - 01:18:12]

**[01:08:15]** Regular contact lenses like you sort of, there's like a little suction pump so that they stay fixed. And then you put the head in like a big kind of like magnetic field. And you can measure the eye movements using that with a very, very high level of accuracy. And I think those probably do give you torsion just by the nature of their existence. But that's a much more specialized piece of equipment.

**[01:08:38]** It's like I would never like, I can't put that in my backpack and I certainly can't put it on you as you walk around the world.

**[01:08:46]** But yeah. So with the current state of eye tracking, torsion is unavailable. There are some ways of doing it. You can't even see them on this video. But there's features in my iris that if you could track those features from frame to frame, you would be able to tell whether they're rotating.

**[01:09:10]** Like, you know, again, look at your eye in a video and when you look at the video and you can see your eye rotating around its visual axis. You will be doing that because your giant human brain is noticing that the texture on the iris is rotating around the black people in the middle. And because you can see it, it means that the information is there in the signal. And so hypothetically, you could design an algorithm that does that tracking on its own. But when the rubber hits the road and you actually try to implement it in a real signal, the answer is it would be a very difficult thing to do correctly.

**[01:09:48]** And you could probably, you know, if you had a 4K image at 120fps in a perfectly, perfectly lit environment, you might be able to get that signal out. But anyways, for the most part, we don't get that signal here and we are roughly speaking, okay with that. I'm not okay with that. It bothers me forever. But I'm waiting for the field to produce something that works.

**[01:10:13]** I have friends that are working on that type of problem and they've made progress. But it is in the case of head fixed in a vise with a giant camera and perfectly ideal recordings for trained subjects who know how to calibrate. We'll get there. It's also kind of a bummer because People Labs is moving away from this kind of classical measurement and they're moving towards a more machine learning solution because their bread and butter is marketing. The people in the world that are buying the most.

**[01:10:44]** Eye trackers are marketing people who are showing. You put the eye tracker on someone, you show them an ad and you see where their eyes go. That's unfortunately where all the money is. And those people don't actually care about the low level empirical aspects of that. So someday Freemo cat might produce eye trackers.

**[01:11:00]** But if that ever happens, know that it was begrudgingly because no one was making the eye trackers that I wanted.

**[01:11:11]** Everything I do, if I ever have to make anything, I'm annoyed about it because I feel like. Because I'm making it because it should already exist and if no one else is going to make it, I guess I will.

**[01:11:25]** Anyway. Where are we at? We're doing stuff. Yeah. So I can just talk forever about any old thing, can't I?

**[01:11:39]** I guess this is probably, this is probably fine because this will give me a little bit more time to clean up the data. Because we're going to look at the data, but we're obviously just like I spent all the time in the sort of philosophy of science space, haven't actually gotten into the fun, the fun crosshairs actually. This is a. You all understand, but next time is already kind of like a halfway. Like it's sort of like it's catch up time.

**[01:12:03]** So I'll have a little bit of extra time to kind of clean up the raw data a little more. So I'll show you what that looks like before we get out of here and then we can talk about it more in detail along with the other catch up stuff. I could absolutely talk about neurons for the full class period, but I don't want to. So I think we can do piece by piece.

**[01:12:28]** Yeah. So here we are with our fun stream of images coming off a camera and from each one we're trying to extract some low level set of data. In terms of dimensionality, this if we're assuming it's an uncompressed image, which it's not, but let's pretend like it is, then the dimensionality of each image is 160,000 degrees of freedom because every data blob of the type image is going to have 160,000 values which can vary between 0 and 1. So the space of possible numbers there is 160,000 dimensions, which is a lot.

**[01:13:14]** So we boiled it down and so in this particular case we're going to boil it down into X and y. And I'm even going to, I'm just going to. I'm not even going to think about diameter because I don't care about that. And so now we have managed to boil down this 160,000 degree of freedom data type down to 2 degrees of freedom, which is way better, way more Tractable. So now we have.

**[01:13:44]** So from this whole data blob we say there's actually only two numbers that we need to define the parts of this that we care about. And that's pupil X, pupil Y and then sort of implicitly frame number.

**[01:14:06]** And these values can Both vary between 0 and 1, where 1 is the width of the image. Or if you want to be more sort of like empirically grounded, it's the number, the width in pixels. So for a 400 by 400 image this number can range between 0 and 400 and so can this one. But because the 400 number isn't going to change and it's kind of, it's like a cumbersome, we might as well just divide everything by 400. And now it's varying between 0 and 1.

**[01:14:44]** And we have two numbers that vary between 0 and 1. This happens to be a square image. Most images are not. Most images have some kind of an aspect ratio like you know, 640 by 480 or which is 4 by 3 or what is that, 1920 by 1080 which is 16 by 9. It's been an embarrassing amount of my life thinking that these were the same numbers because I just forgot how to do, how to reduce fractions.

**[01:15:19]** But these are basically like the two aspect ratios you tend to see in your daily life. So if you are looking at one of these types of images and you have converted from where the width is between 0 and 1 and verticals between 0 and 1, you've now converted a rectangle into a square, which is fine, but just be aware that you've done that. And if you want it to represent the data spatially again, you have to multiply by the blah, blah, blah, blah. Unlikely to come up in your day to day life. But if it ever does, does, they'll say I didn't warn you.

**[01:15:53]** But yeah. So we take each of these data blobs that we call an image, we do some old school computer vision to them and we extract a very, very highly reduced data format in the form of X and Y from each frame. So now this stream of however many bits per second is, goes down to 240 bytes per second. And now this is starting to feel like more like something I can handle. It's still a lot.

**[01:16:27]** It's still 240 numbers per second. That's a lot of numbers. If you had to write them down by hand. But luckily you don't. The machine has your back.

**[01:16:34]** Here we have these nice rectangular friends that are much, much dumber than us, but they're way, way faster. And we can save a lot of time by appropriately divvying up the labor accordingly.

**[01:16:53]** Okay, I'm going to move away. Do anyone have anything to say about giant images of eyeballs?

**[01:17:01]** What more could be said? Lots. Lots could be said.

**[01:17:08]** So, going back into this data bucket.

**[01:17:20]** So here we have again, these blobs, these numbers in the nice, friendly row. And then these numbers here sort of correspond to, like, how many bytes they are. KB is 1000 bytes, MB is a million bytes. D GB is a billion bytes. And they keep going.

**[01:17:38]** Terabytes, picobytes, yottabytes. I don't know.

**[01:17:45]** As I have progressed through my life, you sort of like, I have, like, vague memories of the first time I saw G.B. in my life. And then everything became gigabytes. And then you start seeing pb, which is terabytes. And then it's like, oh, that's a big number.

**[01:17:59]** I think I encountered a peekabyte in my data, or exabyte. I think it's picabyte, exabyte, yottabyte. I don't know. The numbers keep getting bigger, but, you know,

---

### Chunk 9 [01:18:00 - 01:27:59]

**[01:18:00]** Encountered a picobyte in my data or exabyte. I think it's picabyte, exabyte, yottabyte. I don't know. The numbers keep getting bigger, but you know.

**[01:18:16]** Yeah, they have that picture of the person standing by the. The stack. Like it was like 16 kilobytes I think is there. That was their. And I was just like, we have more on our phone.

**[01:18:32]** It's troubling, it's troubling. 1 second per second. Life progresses at 1 second per second, which seems reasonable, but it adds up, I tell you.

**[01:18:43]** Yeah. And actually in a lot of computer worlds you'll still see things like compressed. Anytime you see something unnecessarily squished down to a three letter acronym. That's from back in the era where they were worried about things like how long is your file name and how long is your variable name? Because the number of bytes it takes to write this out was on the scale of numbers you had to worry about.

**[01:19:07]** So there's still a really common. You'll still think instead of writing error they write err. And that's because for a long time I think raw C code that was like a one version of it way long ago. Like you couldn't have variable names that were longer than three characters. So it's like.

**[01:19:26]** Yeah, and now I'm slinging around gigabytes for fun. Just why not?

**[01:19:40]** Yeah, Data. Data, Lots of data.

**[01:19:53]** Yeah. So there's a companion software. So this guy down here, that's Pupil Capture. What's the software I use to record? This one right here is called Pupil Player.

**[01:20:04]** It's sort of like a companion software to do like the calibration and whatnot. I'll talk more about that next time. But suffice it to say I did it and I did some calibrationy stuff. And I don't know why I looked there. This exports folder here, that one right there.

**[01:20:24]** We now have sort of. We've moved now beyond. This is another really common thing in this type of data analysis where there's a distinction in the folder structure, just like conceptually between the data of the recording. This is all recording data. Some of it has different sort of spaces in the empirical life, like intrinsics.

**[01:20:53]** You can actually go back and recalculate that Blinks is actually also kind of derived. But there's a big distinction, the very important distinction between raw data and derived data, calculated data. Raw data is the stuff that you cannot get. Again, like I cannot go back and say, boy, I sure wish I had Recorded this at like a different resolution or a different frame rate. What was happening between frames one and frame two, I will never know.

**[01:21:19]** I could never know. I could get new data of a different thing and get the. And have a higher frame rate for that. But this moment in time, this thing that I measured in the past, this is what we got and this is all we'll ever have.

**[01:21:33]** Other kinds of data, things like blinks, you calculate the blink data from the raw data. So there's some other analysis of looking at the raw recording of the eye and having some method of determining whether or not you think that the eyes have closed in that frame. So things like blinks, that's derived data, that's computed data. If I don't like the way that I calculated these blinks, I can go calculate them again. And I could do that a thousand times in a thousand ways.

**[01:22:03]** And. And as long as I am basing my computations on the raw data, there's nothing really all that precious about this type of data. Unless it takes a long time to process or if my code sucks and I don't know how to. It only runs half the time I push go. This is sort of a different type of thing.

**[01:22:25]** This exports folder here also represents basically everything. Here is a lot of this is automatically computed. But the only. By my definitions, the only actual pieces of raw Data are the MP4 videos and the timestamps.

**[01:22:46]** The exports folder is computed stuff and that's where you see things like Gaze Positions, Pupil Positions, worldtimestamps, CSV. I don't know why we're doing that, but why not this? I always have deeply appreciated of the Pupil Labs guys. They include this Txt, which is a human readable file format raw text that is a description of everything that's in this folder.

**[01:23:28]** So Gaze positions has timestamps, index. Confidence is always a number that we care about. If you're making an estimate, it's like I am 100% confident that that's where the people is versus I'm like 50% confident. This is always a number you tend to get.

**[01:23:43]** And this is another case where you have to know a lot about the system to know how much you should trust this confidence value. A lot of neural network, AI type of stuff. It produces a confidence value, but it's a. Never trust an AI's confidence in itself because it is a classic AI penguin school bus.

**[01:24:13]** No thank you.

**[01:24:20]** Somehow that didn't work. I can't understand why. What was the name of that thing? It was something catchy. Image Confidence There it is.

**[01:24:33]** Yeah.

**[01:24:36]** So these are. This is a classic example of, like.

**[01:24:45]** So what they did is they got. They trained neural networks to recognize certain things, and they got something that could recognize, like, a soccer ball. And then what they did is they went through and they took the image of the soccer ball that was rated as, like, 100% confident that this is a soccer ball. Then you go through each of the pixels, and you start fiddling with the pixels, and you find pixels that if you change their value, it doesn't affect the confidence in the output. Then you just keep running through and just, like, fiddling with the pixels and finding.

**[01:25:16]** Basically, it's called the null space of the prediction until you get to the place where the model is producing 100% confidence that this is a soccer ball, 100% confidence that this is an accordion. And you get to this place where it's like, now we engage our weird sort of goopy human primate brain, and we're like, yeah, I can kind of see that. I understand why you think this is a soccer ball and this is a bagel. It's not, but it is. But all this is to belabor the point.

**[01:25:45]** When you have those, like, AI inferential sorts of things, like the confidence value, it doesn't mean nothing. But if it says 100% confident, just remember this image, this is. This is a nematode.

**[01:26:00]** Great. These little things, they're wild. Okay? And this was probably just enough time to show.

**[01:26:10]** I'll click on it and show the big square of numbers, and we say, ooh, look at those big squares of numbers.

**[01:26:16]** CSV is comma separated values, Also a RAW text format. Anytime you're opening up something in Excel, it's just a formatted CSV or TSV, which is T tab separated.

**[01:26:30]** Don't do that.

**[01:26:37]** So look, big square numbers. Ooh. And so here we have timestamps. So one of the things that makes an eye tracker a piece of research equipment is that it does actually keep track of the timestamps very carefully, and we can roughly trust what they say. So it's roughly.

**[01:26:54]** It's actually not one row per frame because this column is world, like world camera index. So you see, these are all from frame zero. So it's chunked out. This is those confidence values. You can see it's ranging between zero, meaning it didn't detect anything.

**[01:27:09]** 99.99 is as high confidence as it gets. And then sort of some numbers that are lower than that.

**[01:27:17]** And. Yeah. And then norm, pause X normalized screen position X normposy normalize screen position Y. Those are those two sort of XY values that we care about here. You can see they range between 0 and 1.

**[01:27:36]** And then there's a bunch of. This is that diameter value that I don't care about, but other people do. And then there's a bunch of other stuff which basically everything after this line is using the 3D spherical model. So you have different sort of numbers of like, where are you getting ellipse center X ellipse?

---

### Chunk 10 [01:27:45 - 01:30:08]

**[01:27:45]** Basically everything after this line is using the 3D spherical model. So you have different sort of numbers of like where are you getting ellipse center, X ellipse center Y, ellipse axis ab. Because I don't remember how you. I don't really remember like ellipse math, but you need the center and then like minor axis, major axis or whatever angle, diameter and 3D model confidence, it's just like more.

**[01:28:19]** Yeah, it's like a lot of things I've been saying is in that two dimensional space. And then you can imagine once you have that 2D estimate of the ellipse, if you assume that you're looking at a circle on a sphere, then it's going to be more or less elliptical based off of its angle that you're looking at it. And that's kind of all this other stuff is based off of those more complex measurements. Yeah. And that's when you start getting things like sphere center, sphere, radius, circle is the pupil.

**[01:28:50]** And then the last thing I'll say is on that topic of the spherical projection stuff, the thetas and the rows. There you go. Theta phi. Theta phi. So not theta rho.

**[01:29:01]** They do theta phi.

**[01:29:04]** One of them is axis, one of them is elevate. Elevation is the length. Yeah, yeah. So notice there is no row here. Yeah.

**[01:29:15]** So it's azimuth and elevation, which I remember because elevation makes sense and azimuth is the other one. So you only ever need N minus 1 mnemonics for a list of things to remember.

**[01:29:31]** Okay. And that's the end of the class. So I guess while you're, while you're packing up, I'll play this if it plays.

**[01:29:48]** There you go.

**[01:29:52]** I will. Jury duty Wednesday, so I can't. Okay, cool. Good luck engaging in your civic duty. I suppose.

**[01:30:02]** Yeah. So this is the gaze. This is also after the calibration between the eye cameras and the world camera. So the red.

---
