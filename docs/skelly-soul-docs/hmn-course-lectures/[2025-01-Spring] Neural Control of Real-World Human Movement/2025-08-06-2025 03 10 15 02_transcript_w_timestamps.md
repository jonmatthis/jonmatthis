# Transcript: 2025-08-06-2025 03 10 15 02

## Source Information

- **Source Type:** Local File
- **File Path:** `C:\Users\jonma\syncthing-folders\jon-alienware-pc-synology-nas-sync\videos\video_eater_downloads\playlists\[2025-01-Spring] Neural Control of Real-World Human Movement\2025-08-06-2025 03 10 15 02\2025-08-06-2025 03 10 15 02.mp4`

---

**Total Duration:** 01:28:32

---

## Full Transcript

### Chunk 1 [00:00:01 - 00:10:00]

**[00:00:01]** Okay.

**[00:00:05]** Hello, everybody. Welcome back to this space. Hope you had good spring breaks. Spring has broken and you are all inside, which I take is a tremendous compliment. So thank you.

**[00:00:20]** I have brought still more technology, so I brought. This is very exciting. I brought both an eye tracker and a computer today, which is many consider to be the minimum equipment set required to actually record eye tracking. So that's. Let's see, I'm going to pull this.

**[00:00:41]** I don't need that.

**[00:00:47]** All right, so today, as promised, we're going to be doing an eye tracking demo. You can see my eyes very big on the screen. I don't think that will take up the whole time, but I've been historically bad at predicting, predicting how much time things will take.

**[00:01:07]** But I think that what will probably happen is. Let me rephrase that. I know that the recording can't take up the whole time because I can't record that long and have anything to say about it. So what I suspect we'll do today is I'm going to do some kind of catch up stuff and sort of talking about various states of various things and assignments and posters and all that good stuff. And then I'll talk a little bit about eye tracking kind of in general, and then I'll do the actual eye tracking demo.

**[00:01:36]** And then at the end we'll see where we're at. And if not, either we'll be at the end and we'll call it good or we'll spend the last chunk of the day.

**[00:01:49]** I want to introduce you guys to some of the other AI tools that I found recently that are very nice and helpful and maybe beneficial to you in your daily lives. Skellybot is a useful tool for being kind of a, you know, class wrangler. But there's a lot of additional tools people have put out that can do things like search PubMed and search the Internet and look at PDFs and stuff like that. So if there is time, we can take some time to kind of like, you know, work through some of those together or y' all can work on it and I'll go around talking to you, or I'll just tell you about them and you can do them yourself.

**[00:02:27]** Okay.

**[00:02:39]** Okay. Have I pulled this down? Yes.

**[00:02:50]** Yeah. Okay, so this is the schedule that exists. We're on week 10 out of 15, whatever that proportionality is. Two thirds sounds right. And today we're going to do the eye tracking demo.

**[00:03:06]** Next time. This is going to be another kind of like hybrid thing. So my plan is to record the data today And I'm not going to be able to just like spin it around and show it to you because it'll take a little bit more finagling than that. So I will either have a chance to kind of crank something out before Wednesday, or I'll just push it to the next dime and we'll kind of shuffle around the topics that way. In terms of topics, I'll talk about the assignments in a second.

**[00:03:35]** But in terms of topics, this Bees, ants and dragonflies. Those are just like three papers that I really like about perceptual motor systems in insects, which is personally near and dear hobby or sort of topic to my heart for no particular reason other than I think it's cool. So if I can kind of like blast through those three and just sort of show you what the papers look like, tell you the general story, and just kind of give you more like experience looking into the nitty gritties of particular papers. This evolution thing is just like a little sort of a semi canned lecture that I like to do about the broad history of everything sort of leading up to the musculoskeletal and nervous system that you are currently walking around with, which just provides a lot of context to things. Gap filling is intended.

**[00:04:25]** It's kind of just an empty spot. I'll put whatever's in there if it needs to be in there. When things shake out Next Monday we will take. Wait, no, two Mondays from now. The poster upload is Tuesday, so.

**[00:04:41]** So we'll take that one day to have just like in class, kind of like let's go over our posters together and sort of make sure that we're happy with them. Make sure the formatting is all looking good and the PDFs are the right shape and all that type sort of stuff. And then if you're in class, you can do the upload there so that way that can be sorted.

**[00:05:01]** The next day I'm going to talk about my own dumb research history, which is sort of a fair amount there, but I've talked about it a lot so I can be moderately efficient with it. But it's sort of like, especially after the evolution talk, you have at least been officially exposed to the majority of the context that my research has occurred within. So I feel sort of like part of the way that I kind of organize this class is like, okay, what would have to be kind of in the background for me to be able to talk about the research that I do without spending the entire time talking about like the whys and the context and things like that so fun. After that, another semican lecture I like to give on the autonomic nervous system, which is sort of. At one point in this class, I said you have a central nervous system and a peripheral nervous system.

**[00:06:01]** But that's a bit of an oversimplification. Things like the autonomic nervous system could arguably be considered kind of like separate from those two. Not to mention the entera nervous system, the gut, whatever the gut is. And that ans sort of has a lot of ties to things like trauma and ptsd, which I think is a good. Just kind of like a public service announcement like, hey, you're a human person.

**[00:06:26]** This is how your body responds to stress at varying levels. So this is good to know for yourself and also for interactions with other humans who have been shackled with similar strengths and weaknesses. After that. This last Wednesday here, I guess we'll be in April by that point. We'll spend that time doing sort of poster practice.

**[00:06:49]** At that point, your posters will be kind of locked in. Like the physical poster will be, I think, printed by that point. Oh, yeah, it will be printed. Yeah. I'm not sure, not quite sure how that works.

**[00:07:00]** Posters will be printed. So we can sort of either, if we have physical posters, bring them in here, but that might get crowded. Or just kind of like have a bit of practice describing your poster to fellow humans. Just to kind of like give you a little bit of practice and kind of give you some experience, kind of like trying to speak the content of the poster. Because it is a very human and recurring experience to feel like, oh, yeah, I 100% have this in my head.

**[00:07:29]** And then you actually try to do it and you're like, wait, I haven't actually ever produced these sounds in this order before. And it's just, it's a good idea. I mean, no matter what, you'll have your posters feel locked down pat by the end of your poster session. But if we do a little practice beforehand, it will free you up from some. The first couple will always be a bit clunky, but ideally we can do that in a more controlled environment.

**[00:07:56]** The week of the poster presentation itself, I will give you an assignment of, like, which day you're presenting and which day you're observing. If you have a strong preference, like, if there's something else going on, let me know. But otherwise it's just, you know, it's during this class period, so I know you're free. So you're supposed to go to all of the sessions and you'll either be presenting your poster or going around and sort of collecting check marks from people who have going to posters and we'll talk about how to sort of wrangle that. But basically the goal is to both get experience being at a poster, which is its own kind of skill, and also give the person that you're talking to, the person that's presenting the poster, you know, constructive feedback that you kind of need an external set of eyeballs to do.

**[00:08:46]** And then last week we'll do some retrospectives, some wrap ups. I'll show you how to make your own Skelly bot server if you're into that. Talk about sort of some, you know, I will have sort of a finalized ish data analysis of the course and some representations of that and we'll call that good. So I'll talk about the assignments in a second. But in terms of content, is there any thoughts, feelings, questions, emotional helpers related to that?

**[00:09:17]** Seems fine. Cool. Okay. In terms of assignments and assignment like objects I have put up. So I see a number of y' all have already had the midterm chat in the core server.

**[00:09:39]** In the assignments channel there's a mid term chat. And in that channel the bot has been prompted to kind of help you connect your interests and topic to the broader topics of the course. And we talked last time about the prompt has now expanded in complexity a lot. I added a lot of like basically summarizations and sort of.

---

### Chunk 2 [00:09:45 - 00:19:38]

**[00:09:45]** Kind of help you connect your interests and topic to the broader topics of the course. And we talked last time about the. The prompt has now expanded in complexity a lot. I added a lot of like, basically summarizations and sort of condensations of everything that I've been talking about in the lectures of the course. So, and just kind of scanning it, like, does anyone who's had the chat, like, how are the vibes now?

**[00:10:14]** Does it seem similar? Does it seem. Has it gone weird? Does it talk too much or too little or say anything? My scan, cursory scan of what's going on seems good to me.

**[00:10:27]** It's one of those things where I can tell that there's a shift in the way that it talks about stuff in a way that it feels really good. From my perspective, I'm not sure if you would notice the changes that happened because, like, from my perspective, it's kind of like because y' all tend to come in and you say like, hey, I've got this, that and the other interest. And then the bot sort of previously was told about what the class is about, but wasn't given like the details of like the actual lectures. So if you say something like, hey, I'm interested in like sports biomechanics, the bot gives you kind of the. In the way that these things tend to do sort of the belly of the bell curve, like middle of the road, like statistically most likely answer of with this prompting, kind of like answering the question of like, hey, what is biomechanics?

**[00:11:15]** Now that it has this sort of fairly excessive level of pre prompting with the content being like the lectures that I have given, the way that it answers those questions feels much more to me like the way that I would answer the questions, like the things that it highlights, the things that it brings up, the things that it kind of ties in is more aligned with the kinds of things that I personally would. If you asked me, hey, what's biomechanics? I would give you a different answer than if you asked, you know, some hypothetical statistically average biomechanist person. So it's one of those. And it's one of these things, like, there's no way for you to quite know that.

**[00:11:57]** But from my perspective, it's looks better. And so as long as it's not getting weird, if it ever starts just repeating the same word over and over and over and over again, like stuff like that, that is one of the ways these things brains can break. But I think even though it feels like a lot of prompting I think in the scheme of the landscape of this technology, it's not actually that much cool.

**[00:12:25]** Oh yeah. And then there is now a official canvas assignment for that. So if you've already done it, just go in there and kind of check the box I added. Has anyone seen the assignment yet? Like on canvas?

**[00:12:40]** When did you post it? Like this morning? Oh yeah, I was going to say I checked for it. Yeah, yeah, yeah. Like when I say morning I think is a broad term in this weird daylight savings time.

**[00:12:51]** So there should be like an entry for like a URL. Is that right? Is that accurate? Yeah. Ok. Because I just checked the box of URL entry.

**[00:13:01]** That URL is supposed to be like a. Just post a link to the chat so you can get that in various ways. Just right click it. Actually it might need to be the message. Yeah.

**[00:13:14]** So basically any message attached to the chat, either top level or in it, just so that I can see that there. And mostly that is because I realized I don't actually have like, I don't have like a mapping from your discord ID to your student id. So I'm trying to like extract that semi passively.

**[00:13:40]** And I have also now added this poster outline. So in terms of assignments and assignment, like objects, just because things meet deadlines, both the midterm chat and a poster outline. Chat or poster outline, I guess full stop are officially due at the end of this week, meaning like Sunday before next week. You know, I don't please do it, but I don't. I'm not going to like chase you down or ruin your future or anything like that.

**[00:14:20]** And the midterm chat is like have a conversation. Like, you know, you write at least five messages so there's at least 10 total. But obviously you can keep talking if it's interesting. And then the second one here is specifically to help you come up with like the outline form of your paper. So like the things we talked about of like here's my paper, here's the, you know, intro methods, what's it called?

**[00:14:48]** Results, conclusions and kind of like making sure that you know, you know which images you're going to copy paste into things. Don't put images in the chats. I think it might actually break if you do that. I'm not 100% sure what will happen. But if you put a video, if you put an image in there and it stops responding, then just like make another chat and then don't do that.

**[00:15:11]** And this one I was doing some very expedient and elegant prompting to tell it to Pay attention to the assignment. I think one of the things, because the base prompt has gotten so long, when I put little instructions in the channels, it often ignores them. So repetition is a very efficient form of emphasis. So I just basically yelled at it until it started responding, until it started. Because it would be the kind of thing, it's like it would start the conversation just saying like, hey, how's it going?

**[00:15:41]** And I'd be like, is there something you're supposed to do? And it's like, oh yeah, we're supposed to do this outline thing. So I just added this and just copy pasted it so that now when you start to chat it will be like, oh, hey, we're doing this now. So it's just outline thing. Also let me know if its behavior gets weird, but I think it should be fine.

**[00:15:59]** This kind of stuff also is, it's pretty effective, but it's the kind of thing like I wouldn't be surprised if it starts. It's like the personality gets weird in sort of more like traditional like teacher student types of relationship stuff of like not wanting to talk about anything that's not like hard to find in the assignment. But we'll see.

**[00:16:22]** And again the oh yeah, and I'm making both of those assignments that have due dates and all that stuff. The midterm is like official. They're both official assignments. But specifically the poster outline is designed to help you out and just make sure that you have those ideas together. All these things are designed to help you out.

**[00:16:48]** I'm not really designed, I'm not attempting to twist your arm and make you struggle. Just this is all really to help you do the kind of synthesis and integration and kind of like connecting the vague thoughts to your specific interests. And then the sort of the output of the poster outline is you're supposed to ask it for a sort of structured summary that you can copy paste into the assignment, into the campus assignment. And you are welcome to do that just sort of copy pasting type of thing. Don't copy paste from your paper.

**[00:17:23]** But if you take, if you have a conversation and then at the end you say, hey, give me an outline, you can specifically prompt it to like just copy this into the thing.

**[00:17:38]** That's valid. If you don't like its responses, you can either do the natural human behavior of keep telling it to fix it over and over again and never being satisfied with the response, or you can do something crazy which is copy paste it into a word editor and then just change it yourself. Or if you're really feeling saucy. Just write it yourself. It won't take you that long.

**[00:18:02]** And. Yeah, then copy that in. And.

**[00:18:08]** Yeah. And then the next week, the assignment is the poster draft. And that will be here again kind of the Sunday before the upload day. And that is just a singular. I guess I'll ask you for the PDF version of it at that point, but if you have a hard time making that happen, it's fine to just give me the PowerPoint or whatever other format you want.

**[00:18:42]** We'll figure out the PDF stuff in class.

**[00:18:47]** Yeah. And that's again, to make sure that you have something prepared and not ahead of time. We can make sure that everybody's uploading their poster at the appropriate time.

**[00:19:01]** Yep. And then other than that, poster presentation is obviously required. And then there's going to be one last kind of like outro chat, basically the opposite of the intro chat that's just kind of like, okay, now we know who you are. You've been around here, so talk about your experiences and all that good stuff, which is always my favorite. So we'll do that when it comes around.

**[00:19:30]** Okay. Feelings around. That sounds good. Seems okay. Cool.

---

### Chunk 3 [00:19:30 - 00:29:30]

**[00:19:30]** Okay. Okay. Feelings around that sounds good. Seems okay. Cool.

**[00:19:44]** Yeah. I did also put yet another checkpoint into the resources server State checkpoints tab bottom there.

**[00:20:01]** Starting to get kind of interesting, the structure of it. I'm not going to dig into it today, but I was looking through it and it's like it's starting to get to the point where there's enough content in the server and kind of some things have shifted. At some point in the semester, I told it to start mapping keywords in the square brackets, which I'm sure you've noticed, which creates internal links for Obsidian. So that sort of. If you say, oh, hey, tell me about sports biomechanics, it's like, oh yeah, blah, blah, blah, biomechanics.

**[00:20:31]** And it gets those square brackets around it that makes an automatic link to any other conversation that has had that similar tag. So it kind of changes some of the.

**[00:20:44]** Yeah, it certainly makes some interesting structures emerge. So feel free to poke around because it's interesting.

**[00:20:59]** See?

**[00:21:04]** Okay, cool.

**[00:21:23]** All right.

**[00:21:28]** So in this, we have discussed a wide variety of things around neural control of real world human movement and discuss it from a lot of different angles and a lot of the kind of focus on the empirical research associated with that topic of neurally controlled human movement and focusing on both the interest and sort of why you want to study it. Some of the kind of cartoonified stories that we tell around what we know about those systems. Anytime I give you a lecture where it's just like, oh yeah, there's this part of the brain, it talks to that part of the brain that's connected to this sort of thing, and then there's a spine. It has these, this, that and the other properties. Those are the kind of the narrative stories that we extract from whatever the strange activity of research research is.

**[00:22:33]** But those are also obviously abstractions. They're not the type of thing that is precise enough to be nature, even if words were precise. It's just, you know, I can say, oh yeah, you have a spine and it has these parts and it does these roles, but there's always obviously going to be a disconnect between that and your actual spine and the actual configurations of neurons within it. Not to mention the fact that each one of you is an individual and your spines are similar in the same way that all of your faces are similar and all of your hands are similar, which is to say they share roughly the same form, but they are all individually unique and different in their own rights.

**[00:23:18]** So we also. One of the themes I think has been connecting and Sort of really thinking about the connection between the empirical data that we can measure and the tools that we use to record that data and the kind of the different sort of like modes and sort of pipelines of inference and computation that leads us from, you know, voltages on a sensor recorded on a computer to something like, oh yeah, you have a spine and eyeballs and they do this, that and the other kind of thing.

**[00:23:50]** At some point we pulled in a bunch of cameras and that showed you what motion capture looks like, which is a way to measure the kinematics of the body. Kinematics means movement, basically. And today in biomechanics and all that stuff, which is very focused on the, like, the output of the system. So when you talk about the perceptual motor system, you're talking about perception and motor. So perceptual is basically pulling information from the environment or pulling energy from the environment into your system and converting it into various sort of patterns of neural activity processed by various physiological structures.

**[00:24:45]** And then you do something, what we might call cognition, computation, Whatever sort of state transitions happen from the input stage of the perceptual system. Something happens that becomes movement. You make decisions internally uncountably many times per second. And the end result is that your muscles, the motor units in your muscles fire. And that causes your muscles to constrict and that causes forces in the world and you get movement and motion and things like that.

**[00:25:19]** So with something like motion capture, you're pretty much only getting that output. There's ways that you could talk about there being. There are inputs, I guess, being measured when you're looking, motion capture. Because we do have things like proprioception. So there is internal perception of things like my joint angles and the pressure under my feet and things like that.

**[00:25:56]** So that is a part of the perceptual system, but it's not. We don't. That's sort of very different kind of conceptually from the kind of perception you get from vision. Humans are very, very vision oriented animals. We have a lot of our nervous system dedicated to vision and the processing of vision.

**[00:26:17]** And in particular, our sort of personal species level strategy for the kind of precise, fast visual systems that we have is to have very, very mobile eyes with a fovea. A fovea is the sort of the central spot. We showed pictures of the eye that's sort of like there's a place where all the wiring is kind of pushed out of the way. And that's your fovea. It's the center visual field.

**[00:26:48]** It's about the size of your thumb at arm's length. And 50% of your visual cortex is devoted to processing that 1% of your visual field.

**[00:27:00]** And there's obviously lots and lots to say about that. But the main thing that I want to say here is that the way that we use our vision and really the way that you experience your perceptual world Is very, very coupled to your ability to make very fast and very precise image eye movements at a pretty surprising rate. You all now have this experience, this visual experience of living in a colorful, detailed, and precise visual world. Like, you have the sense that you see color from everywhere. You see precision.

**[00:27:35]** You see it's sort of fine edges from everywhere in your visual field. But the reality is that you only actually see that level of precision and that color from that central node of your visual field. And the reason why it feels like you have that level of precision of your entire visual field Is because that information is always very readily available to you. Because if you ever decide that you care about the color in the upper left part of your visual field, you can always make an eye movement to there and have that information within 50 to 100 milliseconds.

**[00:28:15]** So when we are thinking about human movement and we're thinking about human movement in the natural world, the question of eye movement sort of becomes pretty incumbent pretty quickly For a number of interesting reasons.

**[00:28:39]** Eye tracking is really, I think, very interesting and very powerful sort of window and kind of like, I consider it to be kind of. There's like. There's a kind of like a. Like, oh, like, isn't that lucky? Kind of sense of eye tracking?

**[00:28:53]** Like, because. Because we are humans, because we have these very mobile eyes. And because, you know, we're sort of in this classroom people empirically studying that thing, it's very, very convenient that such a cornerstone of our visual, cognitive and behavioral experience Exists in the form of something that is visible on the outside of your body that moves. So in the same sense that you can use cameras to record movement of the body, you can also use cameras to record the movement of the eyes. And because of how fast and how precise your cognitive system is with the way that.

---

### Chunk 4 [00:29:15 - 00:39:14]

**[00:29:15]** Moves. So in the same sense that you can use cameras to record movement of the body, you can also use cameras to record the movement of the eyes. And because of how fast and how precise your cognitive system is with the way that it makes those eye movements, it can be said, and I have said, and I will say, and I'll say again, that studying the movement of your eyes is kind of, it's like a behavioral analog to your cognitive process. Your brain is deciding where to put your eyes in the world. And it's happening at a speed and precision that's pretty far below your level of conscious experience.

**[00:29:56]** Like, I've been studying eye movements and eye tracking since I guess about 11 years now. So I'm pretty tuned in to my own eye movements.

**[00:30:08]** But even still, when I look at the patterns that my eyes make when I'm doing anything of interest, it's surprising. It's surprising how fast and how many eye movements are happening within the space of time of that behavior.

**[00:30:24]** So without any further ado, I guess let's go ahead and take a look at it. So this is an eye tracker. It's a Pupil Labs eye tracker. People Labs is a sort of a nice company. So this eye Tracker costs about $2,000, which in the space of the research world is like very, very cheap.

**[00:30:47]** And all their software is free and open source, which I appreciate. Their new stuff is kind of like this is the People core eye tracker, which seems like they're not really developing it in the way that they used to and they're moving towards more like machine learning solutions, which I, I kind of don't like.

**[00:31:07]** They are faster and the calibration is good. But I don't like having machine learning in my inference pipelines if I can avoid it. So I still prefer these systems, which is more old school classical computer vision. There's three cameras on here. So you have one world camera that sort of facing out, sort of capturing roughly my point of view.

**[00:31:35]** And then you have two eyeball cameras which are pointing in at my eyes here. These are infrared cameras, which is important for reasons I'll show in a second. And then this is an RGB camera. So color camera, red, green, blue, which I don't need to go into detail there, but that is the case.

**[00:32:00]** Yeah, Let me just go ahead and turn it up.

**[00:32:08]** The software is not, I would say the most reliable in the universe, but I think we should probably be able to make it work.

**[00:32:21]** It and the two eyeballs.

**[00:32:49]** Yes.

**[00:32:52]** One of the eye cameras isn't working on this one. So happy to have any that work.

**[00:33:03]** So that's all of you. And then let's look at this one. All right.

**[00:33:18]** That's my right at night.

**[00:33:23]** Overexposed. We'll deal with that in a second.

**[00:33:53]** This is my left eye.

**[00:34:03]** It.

**[00:34:43]** All right, so this is my eyeball. Congratulations. That's specifically my right eye. And you can see.

**[00:35:11]** Yeah, you see there's my shear duct right there.

**[00:35:22]** So the white part is the sclera. And the color, the part of my eye which is normally blue is gray. That's the iris. And then the black spot is my pupil. And as we discussed last time, so the first thing you'll notice that this is grayscale.

**[00:35:44]** It's black and white. That is not necessarily so. That this is an infrared camera, meaning it's sensitive to the wavelengths around 800-1000nm. We cannot see infrared light except very, very barely in very dark rooms. You can see like a little bit of a red glow point.

**[00:36:05]** And the benefit of that is several fold. One is that you'll see these two little white spots here. Those are infrared emitters. So LED light emitting diode. IRAD infrared emitting diode.

**[00:36:20]** Just a color of an LED. And they're not like massively bright LEDs. But if they were a color that I could see, this would be a very uncomfortable thing to wear because it would. Basically I'd be blind. I'd just be like blasted in the eyes with light and I would be able to see.

**[00:36:38]** But because it's blasting light in a wavelength that I am not sensitive to, I don't see anything there. It's just. There's nothing going on. So basically that means that you can have all the benefits of having a bright light in a camera, but you don't blind your participants by blasting light in their eyeballs.

**[00:37:05]** The other benefit of this is that because we are humans, we like to see things. And so one of the main technologies that we really don't give enough credit is that we have artificial lighting in all of our living spaces. And because we cannot see infrared light, we don't care or notice the fact that none of these artificial lighting systems produce infrared light. They're all, you know, we target ways of putting light into rooms that are. That are, what's the word?

**[00:37:39]** In the visible range. So like 450-720nm is roughly the sensitivity of visible light. So as a result, when I look at these lights, you're not seeing reflections in my eyes from the lights out there. However, if I Could you open that real quick.

**[00:38:21]** Ok. You can kind of see. See, there's a little bit of reflection from the. From the window. I need to close the package. I'm sorry.

**[00:38:33]** Yeah, the light's going the wrong direction. Actually, hold on one second, Let me.

**[00:38:41]** Can we see that? I don't know. Maybe not. Yeah, go ahead and close it. Thank you.

**[00:38:48]** The sun has a lot of infrared in it. The sun glows in the black body radiation. You did a great job. Thank you.

**[00:38:57]** So wearing. If I was wearing this outside, this image would be fully washed out. Like you wouldn't like. There'd be reflections of the world on my eyes because there's a lot of infrared in the world. It would mess with the signal.

**[00:39:10]** And when I have done research outside, it's a problem. And I wound up actually.

---

### Chunk 5 [00:39:00 - 00:48:59]

**[00:39:00]** And if I was wearing this outside, this image would be fully washed out. Like, you wouldn't. Like, there'd be reflections of the world on my eyes because there's a lot of infrared in the world. It would mess with the signal. And when I have done research outside, it's a problem.

**[00:39:13]** And I wound up actually making people wear this big, kind of daft pump green face shield. That's infrared blocking. And I'll show you video of that when I give a lecture on that topic.

**[00:39:27]** Yeah. So basically, this camera is equivalent to having a camera in a dark room with a spotlight shining on the thing that you're filming. And everything else in the room is basically pitch black.

**[00:39:47]** So you see a lot of infrared. So a lot of motion capture, like traditional motion capture with markers, uses infrared cameras for the same reason, where you could. Basically, we have, like, infrared spotlights all through the room that do not actually. They're not actually visible.

**[00:40:04]** Yeah. And another thing.

**[00:40:14]** So it's a little hard to tell.

**[00:40:24]** Show this real quick. So there's a. There's a cool effect that's kind of somewhat hard to see. But you can see Kerkinji images.

**[00:40:36]** Kerkinji is one of these jerks that, like, discovered a bunch of stuff. And because in the Western tradition, we love to name things after people because we're narcissists. When there's a person who just like, discovers a bunch of stuff, usually because they invented some kind of a method, and they're sort of like, thanking their names and everything. You have this thing where, like, fucking everything in the field is named Purkinje. So there's a.

**[00:40:57]** There's like, everything in vision is purkinje shift, Purkinje effect, Purkinje this, purkinje that. It's just like, fuck's sake. We just name things after what they are instead of whoever found it first. Anyways, Purkinji images is a thing where if you have a light source. So we talked about Snell's Law, I believe it was.

**[00:41:28]** I'm now standing in there, and visually, there's a huge bright light right there from the projector, but there's no reflection in my eye. So hard to be impressed by something that isn't there. But I promise you, this would not work nearly as well if that was not the case. Yeah. So there's your eye.

**[00:41:47]** A light source coming in from the outside world is going to produce four different reflections. Every time it change, the medium through which it is passing changes density, I think, is how correctly to say that. So you get a reflection from this outside of the cornea, the inside of the cornea, the outside of the lens, and then the inside of the lens. So there's all these sort of weird refracted angles and stuff like that. And if you look pretty closely, you can see there's some.

**[00:42:24]** The ghosty dots you see here are the Purkinje images from those reflections.

**[00:42:32]** Unfortunately, I used to have a version of this eye Tracker that produced 1080p videos at 30Hz as opposed to 400 at 400 by 400 pixels at 120Hz. For research purposes, it's much better to have that faster frame rate than it is to have higher resolution. But for demonstration purposes, I really miss being able to have those high resolution images of the eye.

**[00:42:57]** C' est la vie. Yeah. You can also see my contact lenses constriction.

**[00:43:45]** So I mentioned pupilometry is a field, but pupil study sort of like the measure. Like there's a lot of to do done about this constriction effect, Although specifically not. So this is the one when we think of pupil constriction, we think about. I don't know what you think about, but the main effect of pupil constriction is that if the world is bright, your pupil constricts so that you let in less light. It's sort of like the aperture of a camera.

**[00:44:11]** And it's just one of the many ways that we can adapt and as aggressively as we do to change the luminance in our scene. So you can see stuff at night walking around in the dark, and you can also see stuff in the middle of the bright sunny day. And you can notice the difference if you pay attention. But you can still like, you can operate at a range of light levels that we again, don't really think about as much as we probably should. But it's a pretty dramatic range.

**[00:44:42]** Next time it's like a full moon night, I guess you have to be like out somewhere dark. But yeah, it's. Pupil constriction is one of the many ways that we sort of have that sensitivity. But there's also like effects of things like emotional state and arousal and fear and whatnot. So there's a lot of research that just looks at the people constriction signal as like a measure of behavioral performance.

**[00:45:06]** And I have beef with that whole field, not because I don't think it's a real effect, but because I don't think it deserves the level of attention that it gets. And I think that most people. I think that people study it because you can study it without knowing how to calibrate your equipment. And I just think scientists are. Most scientists are lazy cowards and they should get better at using their tools.

**[00:45:33]** So if you ever meet someone who sights people on the tree, let them know.

**[00:45:40]** Let them know whatever you want. Not my problem.

**[00:45:44]** Okay, so what do you notice? Is there anything that you notice about the eye movements that's surprising about what you would think your eyes look like?

**[00:46:04]** What's that? It's more generally that's. Yes, it is. No, no. Yeah, yeah, no, that's, it's, it's jerkier than you would think.

**[00:46:15]** Because I've been talking about how I support the linguistic effort of attaching meaning to words without. Yeah, creatively, but yeah, it is. The imbuements are jerkier than you would think they would be. And in particular, if you'll notice that there's kind of two types of. There's these ones and then there's these ones.

**[00:46:46]** So there's slow eyed movements here and then there's jerky eye movements here. So right now I'm just looking at each of your faces as quickly as I can, which I can't look at this. I guess I can't look at this screen. But it's sort of the difference there is between what are called saccades, which is French for jerk. So jerkily moving is literally the term that is the correct one, sacades.

**[00:47:18]** And it is.

**[00:47:37]** This is going to be one of those things for like I'm going to talk about these things. I'll do a recording and then we'll come back and we'll see. Okay, look at that thing. Because this is also getting into like a part of the research field where like I am just generally dissatisfied with the offerings of modern science when it comes to being able to look at eye roots and natural behavior. My postdoc advisor, Mary Ajo is one of the sort of, I would say progenitors of the study of eye movements in natural behavior.

**[00:48:08]** And you know, she's on, she's on kind of her own level. And a lot of the field, I mean science in general as we talk about a lot has like a very strong emphasis on like reductionism and sort of like nailing everything down. So a lot of. And I think that's where you get things like pupillometry dominating when in fact it's such a minor part of our visual system. And you get, you know, a lot of the stimuli look just like, like this is a face which is in a glass face, which I promise you, is just being viewed by a participant whose head isn't a literal vice.

**[00:48:41]** So they study eye movements in sort of absence of real behavior, but that's okay, but.

**[00:48:57]** So a saccade.

---

### Chunk 6 [00:48:46 - 00:58:45]

**[00:48:46]** But that's okay.

**[00:48:50]** But so it's a cod.

**[00:49:10]** So we've seen data like this, looking at time on this axis. Then we have, say, eye horizontal position on this axis. So if I'm looking, you look at my eyes here, right? Is that a quote? So it's at one part of the screen, it's at the other part of the screen, one part of the screen, another part of the screen.

**[00:49:38]** And so if this is one part of the screen, then make an eye movement the other side, then jump like that. I jumped that like that. I've got this kind of really, really steep slope because the velocity is very high. This is the same. So this is a form of motion capture.

**[00:49:59]** This is the same kind of thing as studying the body moving around and same kind of idea there. So saccades, if your head is not moving, saccades resemble a square wave. So a square wave is a wave that looks like a squ.

**[00:50:18]** And the slope here indicates the speed. And saccades are very, very, very fast. They are the fastest movement that your body can make. And they are done. The way that we produce them is very sort of interesting and complex and ties in to many different levels and aspects of the oculomotor system.

**[00:50:43]** So the oculomotor system is often considered like, in separation from like the visual system. So it's like a portion of the visual system. And the part when I talk about like the visual cortex, like the thing in the back of your head that's mostly about visual perception. And like your perception of the world, which is dependent on where your eye is pointing at any given time. Like a lot of the.

**[00:51:04]** That part of the brain is split up retinotopically, retinopathic top retinotopic map.

**[00:51:18]** So retinotopy, retina, as in like retina, the map of the copy is map, retina is retina. So a retinotopic map is like a map of a retina. And if this is your field of view, which is not actually a circle, the Wikipedia page about your peripheral vision is a nice method that. But if you think about the center of your visual field, this is kind of map here. If you look into your visual cortex, it's sort of arranged along that kind of a map.

**[00:51:48]** So your visual cortex is kind of defined by your eyeball, like the center of your visual field. And so your ocular motor system's job is to move that center and round to the areas that have the most interesting, important and relevant information to whatever task you may be performing.

**[00:52:10]** Yeah, complicated stuff. There.

**[00:52:14]** Yeah. So saccades are what we would call fast eye movements. And then there's also these other kind of movements, which are these ones. So these are slow eye movements. Can anyone.

**[00:52:27]** Why is my. Why am I. Why are my eyes moving right now?

**[00:52:36]** So I'm. Right now I'm looking at you. My eyes are looking at you. My retina is extracting information. And now my eyes are moving all over the place even though I'm still looking at you, because they are in my head.

**[00:52:51]** Right. We move our heads a lot. Not as much as birds, but a lot. A fair amount, a percentage of the class just look like, oh yeah, you do.

**[00:53:06]** And when we talk about opsins and like the things in your retina that sort of have. That absorb light and they change shape and how that whole sort of retina top, that sort of cascade occurs.

**[00:53:24]** And one of the main things I've mentioned that is that that process is relatively slow. Slow meaning it takes, you know, a dozen milliseconds to operate and then longer than that to sort of clean itself up. That's why you get like after effects, you look at bright lights. So what that means is that in order for your eyeballs to be able to extract the precise information that we want them to extract, they have to be remain fixed in the world relative to the thing that you're looking at. So I'm looking at someone in the distance and I'm moving my head around.

**[00:53:57]** In order for me to be able to extract the information that I need from that area, my eyes have to stay fixed relative to that point. So your eye muscles have kind of like a gimbal situation, oculomotion muscles.

**[00:54:25]** So your eyes have this kind of a gimbal shape where they have two muscles that control the upward downward movement, two muscles that control the side to side movements, and then two weird lobes on top that control torsion, which is the rotation around the visual axis. And those eye muscles do multiple jobs. And one of them is to make what you can think of as kind of information gathering saccades. So if I'm looking at you and I'm sort of, while I'm looking at like, oh, I wonder what that clock says. Even though it's been stopped incorrectly the entire semester, I might make saccade up there.

**[00:55:00]** So I'm looking here and I look over there and look back. So looking this direction at whatever I'm looking at, back up to the cloud back here. So that's like an information gathering. Or I could be looking at the clock and kind of bouncing around. And then you get this sort of movement which is like a stabilization, slow stabilization kind of gimbal.

**[00:55:19]** Like if anyone's ever played with like a drone, like a quadcopter drone and they have the camera that moves, that's a symbol, that's a two axis gimbal. This thing here is a two axis gimbal. We have a three axis gimbal because we also have this axis there. So this one, this is called torsion.

**[00:55:45]** And it's basically as you. That's that optical axis rotation that you get from those weird top and bottom, what are they called? Interior and superior open muscle. And it's a relatively like. What's the word?

**[00:56:08]** The range of motion is pretty constrained for that. I think it's plus or minus 7 degrees. And you can see as I move. So if I move my head a little bit, you can see my gaze kind of rotating around that optical axis. Trying to keep the image on the back of my retina as close to stable as it can.

**[00:56:27]** But you can also see if I keep rotating, it kind of ticks over.

**[00:56:33]** That's basically it gets to its limit and then kind of gives up and ticks back. So tick, tick, tick, tick, tick, that's fine.

**[00:56:44]** And you can do this yourself. Like next time you get a chance, just have a turn your camera around to face you and kind of like look at your eye and you can kind of see some of these things. However, what you will not, you can see. I thought you were just shocked that the existence of forward facing cameras.

**[00:57:10]** Yeah. So you'll be able to see the slow eye movements. You're looking in the mirror or in the camera or something like that and you move your head around, you'll be able to see the eye movements and you'll start to. It's one of those things that's like. If you've never thought about your eye movements before, congratulations, you're going to be thinking about them a lot.

**[00:57:29]** And it's just kind of sort of a fascinating thing. One of the things. So you'll be able to see these slow eye movements. But what you will not be able to see somewhat strangely is the saccades. Like if you're looking at the camera, maybe with the phone camera.

**[00:57:45]** The phone camera, because it is slightly delayed from reality, you might be able to see it, but in a mirror you won't be able to see a saccade because your vision is suppressed when you're making a fast eye movement. So when your eye is moving at this speed, it's so the movement is so fast that the opsins in your retina have no time to do anything with it. So even if you. So one of these sort of complicated things where, like, even if you were able to see during that time, you wouldn't see anything of interest, it would be blurry. It'd be kind of like if you focus on your finger and move around, you can see the world kind of like blurring out and sort of smearing out in the background.

**[00:58:28]** That's what it would look like if you could see during this cod. But you can't, because it appears that your visual system suppresses it. Like, you could be having a perceptual experience, but something at some level. And anytime you talk about the nervous system at this level of abstraction,

---

### Chunk 7 [00:58:31 - 01:08:30]

**[00:58:31]** But you can't, because it appears that your visual system suppresses it. Like you could be having a perceptual experience, but something at some level. Anytime you talk about the nervous system at this level of abstraction, just understand, I'm like, I'm trying to scrape what I know about a pretty deep subfield which is fairly murky in itself. It's appeared that your visual system has adopted a strategy to not process visual information during a saccade.

**[00:59:03]** Because. Because is also a very dangerous word. Because there's no point. It's not useful information. And so your nervous system does other stuff during that time.

**[00:59:15]** There's evidence that during that sort of trans saccadic period, trans, like during transition, saccad, whatever, that the visual system is kind of preparing for what it thinks. Thinks is going to be under your phobia when your eyeball gets to that point. Because eye movements are kind of a predictive thing. Like, because we do have a fairly broad peripheral field. When I make an eye movement to the clock, I expect to see a clock when I get there.

**[00:59:43]** Because I can see the little white patch, I know that there's a clock there. And so there's evidence that like, you're down to level of like B1 primary visual cortex. There's kind of a preparatory thing happening where the visual system is expecting to see, assert the stimulus that it sort of sees in its periphery, which both makes it much faster to process once you get there and also makes it much faster to process if something goes wrong. So if I'm expecting to see a certain thing and then I wind up seeing something else, that mismatch is triggered much more quickly because of whatever weird magic is happening during the trans saccade period.

**[01:00:23]** And yeah, so these eye movements, slow ones, are mostly driven by vor, which is arguably my.

**[01:00:48]** My favorite reflex.

**[01:00:53]** Vestibulo ocular reflex veolar.

**[01:01:00]** I've certainly mentioned this a little bit, at least at some point. But VOR is the connection between your vestibular organs, which is what people often call your inner ear, your eyeball. And so basically these movements in my eye are directly canceling out the movements of my head. So if my head moves left, I maintain sensation. When my head moves left, my eyes move right and vice versa.

**[01:01:31]** And so the two cancel each other out, which is an effect that I arguably, the reason why I got this job is because I figured out how to sort of take advantage of that coupling to. To calibrate eye trackers into motion capture systems. And that's sort of part of the technical basis of like the laser skeleton stuff at some point.

**[01:01:53]** But it's an extremely low level and extremely old reflex. And your vestibular organs are these very interesting things. In the back of your head there's these fluid filled canals that basically when you move the fluid, like gel fluid, has inertia. So if you move your head, it takes a second for the gel to catch up. And there's these hair cells that are sort of ingrained in that gel.

**[01:02:26]** And so as the gel wobbles, it is picked up by the hair cells. And that's part of how your brain, part of how your nervous system tells your head is moving. It's a specifically head centric measurement that tells you the 6 degree freedom acceleration of your head. So translation and then rotation. You have these like semicircular canals which are literal like circles of goop that are sort of.

**[01:02:56]** They look kind of like this semicircular canals and they measure rotation in all three directions. And so if you ever, not that any of you would, but if you ever like drink alcohol and then lay down and get the stuff sins, it's thought to be related to these things because when you drink alcohol, the density of your blood thins ever so slightly. And your body is very, very sensitive to that type of thing. So if you're walking around with your eyes open, you can tell that the world isn't moving because your eyes are pretty good at doing that kind of stuff. But when you close your eyes and lay down, the only signal that you have telling you whether or not you're rotating is these fluid filled canals which have just had the density of their requisite fluid lowered in an artificial way.

**[01:03:58]** So that is why, if you ever have the experience of drinking alcohol and lying down and feeling like you're spinning, it's because your body starts relying on the vestibular organs and they start. And they're giving faulty information because you have altered the density of the fluid in the blood ever so slightly. So if that ever happens, just open your eyes, grab onto something, give your body any kind of a clue that you are not in fact spinning and hope for the best. And also, don't drink too much. Drinking a lot isn't cool.

**[01:04:33]** It's just like. It's not that it isn't cool, it's just like lame because you just become a burden around you, but they forgive you. Probably.

**[01:04:44]** I would.

**[01:04:50]** Yeah. Okay, cool.

**[01:04:55]** One other piece of. So I. You have the fast eye movements of saccades, these kind of jerky movements, jerky, jittery movements.

**[01:05:08]** You have the slow movements of vestibular ocular reflex. And there's a couple others too. A relative newcomer to the sort of pantheon of eye movements in the sort of evolutionary time is this one. So notice that my head is not moving and yet my eyes are now still going to have a nice smooth movement. This is calves blue pursuit and it's me tracking the.

**[01:05:39]** Basically I'm tracking an object in the world. So I'm looking at my finger and tracking it smoothly. And because of that sort of like visual anchor point, I am able to generate smooth eye movements. But if I were trying, I'm just now going to do this on the back wall. I'm going to attempt to make a smooth eye movement from the left to the right.

**[01:06:00]** And I can't necessarily, I can't do it because I'm sort of trained in this stuff. But even if I, as smooth as I try to make it, you'll notice that I'm actually just making a series of saccades from one point to the other. But if I track my finger, I can track it smoothly across the back wall. So they're called smooth pursuit eye movements. And they are strange and somewhat mysterious thing.

**[01:06:28]** You cannot do them without a visual reference point. Like your visual system just will not move smoothly like that. There's too many mechanisms that sort of clamp down on the visual environment that you're looking at. But if you're looking at something and tracking it, there's feedback loops there that allow you to make smooth eye movement. An exception to that rule is that apparently you can do that tracking without a visual reference point.

**[01:06:57]** If you're in a completely pitch black room and you're tracking your own finger. So if you're in a room moving your finger, it's pitch black, so you can't see your finger. But somehow you're able to make that kind of like connection of what's called efferent's copy of the movement of your limb is able to smooth move out the eye movements in a very sort of, I would say, interesting and mysterious way.

**[01:07:32]** All right, so we got 20 minutes left and I need probably the last 15 minutes to do the actual recording. Any questions, thoughts, things you want me to see me do?

**[01:08:03]** I think that you can kind of feel your eyes so it doesn't necessarily roll back in your head.

**[01:08:10]** What's that?

**[01:08:15]** I don't think it does.

**[01:08:24]** So what I will say is that blinking is a strange and interesting behavior. It.

---

### Chunk 8 [01:08:15 - 01:18:06]

**[01:08:15]** I don't think it does.

**[01:08:24]** So what I will say is that blinking is a strange and interesting behavior. And, like, the way that we choose when to blink is also kind of like strategically aligned within the eye movement in a way that sort of feels like the purpose of a blink is that these are mucous membranes. They have to stay wet. If you keep your eyes open too long, they dry out. And so we blink to sort of re up the tear film that keeps your eyes sort of happy.

**[01:08:59]** And it is also the case that if you blink, you are blind because your eyes are closed. And there's interesting effects where if you give people difficult tasks, they time their blinks to happen during big saccades. So first of all, if you're doing difficult task. So like, the classic study is on pilots. You have, like a pilot flying a plane, they blink at a normal rate, but during takeoff and landing, they basically stop blinking.

**[01:09:29]** And so when they're coming in for the landing, their eyes stay open. And then when they actually get to the point where they're now safe, they go blink, blink, blink, blink, blink. And I noticed that when I was having people sort of walk on the rocky terrain, like, when they are walking in the terrain, they make very, very few blinks. And when they get to the end, they blink, blink, blink, blink, blink. The exception to that rule is that if you're making a big saccade, like when they're walking and they look up to see the target and look back down, during that period of, like, a big saccade, people will blink and they'll even half blink.

**[01:10:01]** So, like, they'll, like. While my eye is moving because I am blind during that saccade anyway, somehow my nervous system knows to time the blinks during that 50 milliseconds of. Of dead. Of dead time, which is just, yeah, wild and interesting stuff. There's omnipot stuff.

**[01:10:23]** So I don't think our eyes. We don't. It's just not. There's not enough time to move really, during a blink. But it is shockingly complex behavior.

**[01:10:34]** Everything is so complicated. Blinking, blinking, blinking is complicated enough for, like, multiple careers. Like, this is why I like insects. They're small. It seems doable.

**[01:10:48]** Yeah.

**[01:10:53]** Okay, I should get started before I can show more stuff later.

**[01:11:03]** Okay, so now I'm going to do a quick recording that we can. I'm going to analyze later because as everybody knows, when the record button turns on, you get significantly dumber. And that is magnified by the number of cameras. So it's important to plan ahead. So I'm going to start by doing calibration and then horizontal and vertical movements.

**[01:12:12]** I guess we got that smooth suit and then fun stuff then.

**[01:12:29]** Oh, that's not going to it.

**[01:13:16]** Okay, so let's go ahead and get started.

**[01:13:32]** Okay, so we are recording. I'm going to give it a second at the start so we can sort of fill up the imodel which we'll talk about later. And then I give it a calibration scene which is. Okay, so try that again.

**[01:14:10]** Okay, so give it a second at the beginning to sort of stabilize its model job and then calibrate the eye tracker.

**[01:14:30]** So much more horizontal movement than there is for vertical. Okay, so now that is not calibrated but I have the information in the video that I need to calibrate it. And now calibration. Horizontal. Okay.

**[01:14:46]** Yeah. So horizontal saccades. So looking at from finger to finger, vertical saccades controlled by a different part of your visual system. Turns out apparently the separation between horizontal and vertical sacans. I don't know much more than what I just said so vor which I'm already doing that.

**[01:15:08]** But this is something that may or may not work out.

**[01:15:16]** That's fine. Smooth pursuit. So now I'm going to do the thing I did before. So I'm tracking my finger on the back wall and that will be nice and smooth. So vertically, why not a little bit busted?

**[01:15:33]** Because I'm like looking at the UI camera for that one. And now I'm going to try to make the eye movement without that. And when we get the data back you'll see that I fail. Okay, now we do this. So brightly colored balls with a visual loader task.

**[01:15:52]** This is one of those things where I throw the ball and I have some information about the throw and that defines a trajectory. Finds a rigid trajectory in space ballistic. But my information about that trajectory is noisy because it's dependent on like weird sensations from my hand. So what happens is at least a natural tendency. You don't have to do this eventually but we tend to look at the apex of the throw.

**[01:16:20]** So we find we saccade over to where the we saccade over to where we think the ball is going to be and we track it up until it sort of gets to the apex. At that point we have all the information we need to put our hand in the right location and we're good to go. So we'll see what that looks like. And I should read something. Okay, let me read something.

**[01:16:54]** Okay. The vestibular rector is the reflex that goes gaze in the head and the eye movements and blah, blah, blah, blah, blah. Gaze is held steady on location, for example, and the head moves to the right, since that is there. Blah, blah, blah. Okay, I'm done with that.

**[01:17:13]** That's as much reading as I do right now. Reading is like another classic eye tracking task. And, yeah, you can still read with this amount of available cognitive information or cognitive capacity, I guess. Okay. Anything else to do before we turn it off?

**[01:17:39]** No, I think that's good. Okay. And.

**[01:17:49]** Okay, cool.

**[01:18:02]** All right, let me just make sure that I got that data. I didn't do anything wacky to it.

---

### Chunk 9 [01:18:02 - 01:28:32]

**[01:18:02]** All right, let me just make sure that I got that data. I didn't do anything wacky to it.

**[01:18:16]** Pretend this is the last time. I thought this was a different class, but same demo. This is the one that I turned off. This is the one that I used.

**[01:18:31]** Okay.

**[01:18:35]** It's upside down. Because the right. Just the nature of the geometry of the left and right eye, one of them has to be flipped, but the data doesn't care. And also, you really can't see it on the screen. It looks better on this data will be available still at some point.

**[01:18:58]** Cool. And also, so, yeah, this is just like a fun visual of the geometry that's being solved for the eye tracker.

**[01:19:25]** So first of all, this eye tracker does not track torsion. No eye tracker that I'm aware of tracks torsion.

**[01:19:34]** So you'll see the pupil, the sphere kind of just like spins around in its axis because it's just an untracked dimension. Also kind of noisy, but that's the way it is.

**[01:19:49]** This is another kind of thing that in the field, there is a completely incorrect belief in many sort of areas of traditional neuroscience that, like, torsional items are just, like, not behaviorally relevant and don't really sort of happen in an interesting way. And there's sort of arguments that they don't basically give people a pass to ignore them, which are really predicated on the fact that most of the times. So if your head is not moving, you don't need torsion, but if it is, you do, and sort of hard to describe. That's a deeper beef that I have time to get into right now. But basically the algorithm that's being used here is it's assuming that your eye is a sphere, which it's not, but it's close enough.

**[01:20:32]** And if it seems that your pupil is a circle on the surface of that sphere. Sphere. And so if that is the case, and you see. And if you see the dark circle, which is part. That's being circle there, and you're seeing.

**[01:20:53]** If it appears to be a circle, then you must be staring straight down pupil. You see it as an ellipse, then you must be seeing it from the side. And so this noisy thing is based off of that geometric assumption.

**[01:21:13]** Let me see if this works. I feel like they disabled this at some point, but if they didn't.

**[01:21:20]** Image algorithm. Yay.

**[01:21:28]** So this is a representation of the calculation that's actually processing the image. The blue is the everything that's below. I think this. So this is a measure of the luminance of the pixels, brightest, brightest, darkest. I think the bottom is the darkest.

**[01:21:51]** And so this is the blue are the pixels that fall within this bottom range. This is called a dark pupil detector. It's detecting the dark of the pupil and then you can see it gets a little confused once I go to the edge because there's. It gets dark, which I checked this beforehand. I would change the exposure.

**[01:22:11]** And then the yellow is the brightest. Tracking. So that it's also. It's tracking that reflection of the infrareds of the iris. They're called the cornea reflection.

**[01:22:23]** So it's sort of a part of the algorithm there.

**[01:22:30]** Yeah. And then once it detects that dark splash and says, okay, that's probably the pupil. And then it fits in ellipse within that dark patch and then it assumes that's the pupil. And then it does that 3D conversion thing. And then the result is the data that I will show you either next time or this time after that.

**[01:22:55]** Yeah, there you go. That's eye tracking in a nutshell.

**[01:23:01]** It's a fun thing. The data is super rich, like problematically rich, like motion capture data. So motion capture data is also very rich, but it's also higher dimensional. So like it's big blobby data with like a complicated movement. This is.

**[01:23:16]** The data is a low. It's lower dimensional, meaning that you get X and Y position of the gaze in the screen per frame because we don't measure torsion. The signal that you get out of this is just literally like up, down, left, right. So it's a much lower. Just lower dimensionality.

**[01:23:38]** But it's so much of it and it's. So the precision is there, complexity is there, and it's kind of like it's a more abstract. Like the goals are more abstract. Like you don't have the benefits of like Isaac Newton telling you what physics are or what mechanic. Like what mechanical.

**[01:23:56]** Sort of. Yeah, like you don't have like the three laws of motion. You have Snell's law, and that's about it. You have Snell's law and the assumption that we have a phobia. So that's what we're trying to point at the things that we care about.

**[01:24:07]** And then for me. So this is where I think Mary, my. Mary Hayhoe, my advisor, like a lot of her claim to fame, I think in terms of like the. The research sort of umbrella that she has built is the conception of task being a driver of eye movements during natural behavior. So if you can figure out what the task is, that can Help you understand what the eye movements are.

**[01:24:34]** So in this case, that task can be very abstract. That's why things like juggling are nice. Because this is a very. It's a fast, it's repetitive and it's success and failure is obvious. And this.

**[01:24:46]** Right now my task is give a lecture to a room full of people. And so if you look at my admins, I'm going to be basically face to face, trying to figure out like saccading from face to face, trying to figure out like, you know, how we're doing, you know, what's landing, what's not.

**[01:25:04]** And yeah, I will say this before we go, for history's sake, humans love faces. Like our favorite thing, Yarbus. And if you're looking at a person, I guess depending on which way they're facing, we mostly look at faces. It's our absolutely favorite thing in the world to look at is a human face. And if you put it a human face in a visual scene, people just naturally look to it.

**[01:25:33]** And then when we look at it, we tend to adopt this pattern of like, eye, eye, mouth, eye, eye, mouth, eye, eye, mouth. I've seen this, like, there's like a weird trend in the sort of the pseudoscience of the Internet. People say talking about like, oh yeah, the triangle method. You got to look at your eyes and look at the mouth and it's like as if it's like a strategy that you can like, some people have seen this. I promise you, that's all anyone's ever doing when they look at you.

**[01:25:56]** And that's all you're doing when you look at anyone. Eye, eye, mouth. That's what we care about because that's where the information is. That's where all that's sort of. There's also an unfortunate reality of like, if a person's not looking at you, there's another part of the body that you tend to look at which you can put all sorts of information onto that.

**[01:26:15]** But it's also the most informative part of the back of a person. Like, it tells you a lot about where they're moving. But it does mean that I've had to learn how to design experiments that tend not to have humans facing away from the participant because it's just not polite to measure that.

**[01:26:33]** Yeah, this is from Darbus 1967. They just showed people faces and then showed them pictures and said things like, you know, what do you think people are doing? You know, what do you think their socioeconomic status is? Like, you know, what era is this image from. And then based off of the instructions, you get different eye movement patterns looking for that same kind of information.

**[01:27:02]** Eye tracking used to look like this. So in like the weird eye contact lenses space, this is like the stuff that my advisor got her degree doing. Like you would put like a suction cup on your sclera with a, with a mirror on it, like a tiny little mirror, like attached to your eye. And then they would shine a bright light in your face so that the mirror would shine onto a projector screen. Then they would film the projector screen and then track the dot there and do all the geometry to sort of figure out back what was going on from the eye.

**[01:27:39]** So we are grateful in using like apparatuses like this, which. This is actually what a lot of visual neuroscience looks like. It's like put your head in the vise and then point. This is like a high principity, this is an old feature, but you still see stuff like this in the modern era. And these cameras are the kinds of cameras that we can make these days.

**[01:28:01]** Very, very high, very high resolution, very high frame rate. Okay. And that's all the time we have today. Perplexity AI and Notebook LM, that's. Those are the tools I particularly like NotebookLM as a Google product.

**[01:28:17]** Just go there, sign with your whatever, Google account, and then just like ask it about your paper and tell it that it can search PubMed for you and have fun. It's a very powerful tool. Okay.

**[01:28:30]** And that's it. Thank you. See you Wednesday.

---
