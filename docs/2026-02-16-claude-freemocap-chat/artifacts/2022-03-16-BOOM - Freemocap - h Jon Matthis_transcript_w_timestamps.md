# Transcript: 2022-03-16-Bonus Episode Your Friendly Neighborhood Markerless Motion Capture with Jon Matthis

## Source Information

- **Source Type:** YouTube Video
- **URL:** https://www.youtube.com/watch?v=kw3hYndzzac
- **Video ID:** `kw3hYndzzac`

---

**Total Duration:** 00:39:08

---

## Full Transcript

### Chunk 1 [00:00:00 - 00:09:59]

**[00:00:00]** Welcome to Biomechanics on our Minds. My name is Melissa Boswell. And I'm Hannah O'. Day. And we're PhD students at Stanford University.

**[00:00:07]** This podcast is brought to you by the International Society of Biomechanics. It's time for boom. Welcome to boom, where we have biomechanics on our minds. Boom. Boom.

**[00:00:20]** Boom. Boom. Boom. Boom. Boom.

**[00:00:24]** Boom. Boom. Boom.

**[00:00:29]** Welcome back to boom. Today we're talking with Jonathan Samir Mathis, who is a tenure track professor of human movement neuroscience in Boston, Mass, hailing from my hometown, and he's also the founder of Free mocap Foundation. And we're so excited to have you. Jonathan, thank you so much for joining us. Lovely to be here.

**[00:00:46]** So can you tell us when you first knew that you wanted to be a biomechanist? I mean, I guess I'm a biomechanist now, but my bachelor's degree, my first degree was in philosophy. And I consider myself more of a neuroscientist than a biomechanist. I studied the part of neuroscience where we move our bodies around. It's a bit of a.

**[00:01:05]** The path is a meandering one, but it went from philosophy to kind of like visual neuroscience, which got me into specifically studying the visual control of locomotion. And then I kind of realized at that point that you can't really plenty try, but you really shouldn't study the visual control of movement if you don't know about the actual movement. And if you want to know about the actual movement, probably going to have to talk about physics. So I kind of, you know, backed my way into biomechanics just as sort of the need arose in my early grad school as a philosophy major trying to learn how to do science and kind of realizing it's like, oh, there's this whole area that I need to know about. And because I didn't know anything, it was just as easy to learn half of vision science and half of biomechanics as it was continuing with your normal regular student.

**[00:01:53]** Yeah, I can see that connection there between neuroscience and then studying human movement. I'm curious about the connection from philosophy to neuroscience. Well, it wasn't necessarily planned. I mean, I was studying sort of philosophy of mind, philosophy of science in my undergrad days and always kind of had a fascination, you know, in retrospect, a fascination with kind of like that interface between, like, you know, what am I and what is the world, which kind of winds up being, you know, if you really want to pare it down into like a study Able question that winds up being perceptual motor control, mind body problem stuff. I actually applied to go to grad school for philosophy and by grace of God was not accepted into any of the programs I applied to and then wound up working at an autism research facility for a year and was like, oh wow, data is cool.

**[00:02:43]** You like say things. Yeah, I do, you know. Another slew of graduate programs in cognitive science was the tag I was looking for. And I was planning to study language and then accidentally one of the places I applied for didn't have a language person. So I didn't notice that until it's like writing the personal statement and I was looking through the list of like, well what else have they got here?

**[00:03:08]** And there was a guy named Brett Fajin, he studies visual control of locomotion and sounds pretty cool. And that's the only one that I got into. So now I found myself studying visual control of locomotion and what is that, 13 some odd years later, skeletons in my office and robotics and professorship and whatever. Like, well it's cool to see you go and be open to all those different parts of your journey. Like you said, by the grace of God, you weren't in philosophy and you really listened to the different like things that I think and I'm like, I haven't hit the ground yet.

**[00:03:42]** So you know, here's hoping. Well, you listen to all the awesome things. We're looking at your, you know, on video in this video call, your background of tools and skeletons and I see like a draw mannequin over there. Can you tell us clear you're a person of many interests but we'd be excited to hear about some of the projects that you're working on right now. So the Free Mocap was the big one or whatever, but kind of like a traditional line research because the Free Mocap is like effectively a Covid project that's kind of, you know, come to where and the air.

**[00:04:12]** So like I have like a traditional line sort of NIH funded research program on visual control of foot placement during locomotion and that was sort of how do you get the visual information that you need to put your feet where it needs to go. And we have some projects brewing with that sort of fun, like augmented reality, projected ground planes and large scale traditional markered motion capture and integration with eye trackers and laser skeletons, data types, which is what I call this like full body skeleton data with like lasers shooting out of the eyes showing where people are looking. And I have Google, whatever my name is, I Have videos. And that was kind of, that was the traditional line research that I was sort of, you know, fully planning to devote everything to in roughly January of 2020, which is in retrospect a very funny time to do planning. In addition to that line, there is the Free mocap project which is, you know, free open source markerless motion capture system for everyone.

**[00:05:09]** And we're going to be talking about that. I know, but sort of like in that space we're kind of working. There's like a bunch of stuff going on with it. One is like the core software development, like making the thing, making it good, making it better, making it work, validation studies, sort of comparing it to my like you know, gold standard classical motion capture system. Recordings, connections being drawn from that to like the animation community through open source softwares like Blender and sort of making sure that the thing that's being built is useful for both for scientists and clinicians as well as like animators and game designers because they need MOCAP2 and also just like it's now at a place where like it works and you can do cool stuff with it if you know how to use it.

**[00:05:48]** So we're also doing cool stuff with it, which is pretty fun. So actually like using it to do researchy kinds of explorations. That about fills my day. That's it sounds like enough to fill a few days. It's interesting to hear your start to Free Mocap as a Covid project.

**[00:06:08]** I'm curious if you could share more about what during that time inspired Free mocap and then when did you realize that this was turning into a bigger project than you expected and you wanted to commit more time to it and really see it advance. I've been using motion capture for research since 2008 when I started with Brett Fajian of RPI who's a wonderful person and a great advisor and he had sort of inherited this motion captures like a Vicon motion capture system which is probably the biomechanics angle is probably from that. So I've been working with mocap for a while and specifically I think like the start of the idea of free mocap came when in 2017 when open pose released their, you know, they did like a. They had like a siggraph paper. What I consider to be like the first like viable markerless mocap thing that I have seen which is, you know, for, you don't know, it's just, you know, traditional motion capture, marker based motion capture.

**[00:07:05]** You put markers on the person, you specialize cameras to track them and then Yada, yada, expensive software. Openpose released this demo video and it was just RAW videos and, and convolutional neural networks, you know, trained on COCO dataset or whatever. And it would draw a skeleton over the person, like, you know, joint centers, connected dots, full body, hands and face of just raw video. And I remember seeing that, thinking, oh, you could build a motion capture system out of that. And then proceeded to like, forget about the idea for about four years.

**[00:07:38]** I mean, I played with it a little bit, but it was like, limited and like, I didn't really have the technical skills or the time to really like focus on it. And then when I got to Boston to start my profess in summer of 2019, which in retrospect is a funny time to start a professorship, I had a whole plan based on the traditional visual control of locomotion thing. And I was waiting for some equipment to come in. I was waiting for an eye tracker to come in. And the student that I had hired to work on the eye tracking part of it needed something to do.

**[00:08:06]** So I was like, hey, why don't you pick up this old open pose motion capture idea, get some pros and work on that. So it's like a stop gap till the eye tracker came in. And that was in January of 2020. And then the funniest thing happened and all of my actual research plans went out the window. I had a whole plan, I hired a bunch of people, we're gonna like build this equipment, do the foot placement thing.

**[00:08:30]** And then all of a sudden the world shut down and I lost access to my lab for an unknown amount of time. So I was thinking, well, I don't know when we're gonna get back. I don't know. And I gotta do something. These students need, you know, they need things to do, I need things to do.

**[00:08:46]** So the what at the time was called the GoPro Motion Capture Aspect of the lab became front and center. That's what we were working on. So that's kind of when it started. And then it has evolved very in a very interesting way since then. Where kind of like as the sort of the first phase of like the GoPro open pose mocap thing was starting to like achieve like viability.

**[00:09:10]** Like we had, we had reconstructed a skeleton. It was a lot of work, but like, it did work. So it was like, okay, this is a viable option. That was sort of roughly around like summer of 2020. So like George Floyd protests and like all sorts of like, you know, world is burning kind of feelings, kind of just like, you know, rising in equities and like, what.

**[00:09:28]** What are we going to do about it? And so sort of we were all this, like, deeply introspective state about, like, wait, like, what am I doing? Am I actually giving more than I'm taking? I don't think I am. So while I was sort of ruminating in those thoughts, I live my life through YouTube tutorials as a philosophy major.

**[00:09:43]** Turn, like, whatever. Science. Yeah. Self education tools. Yeah.

**[00:09:47]** The only real form of education. And I was, like, using. Looking at Blender, which is like an animation software, and I had been looking for a really long time trying to find tutorials on how to load motion capture data into Blender, and I wasn't.

---

### Chunk 2 [00:09:45 - 00:19:44]

**[00:09:45]** Yeah, self education tools. Yeah, the only real form of education. And I was like using, you know, looking at like Blender, which is like an animation software. And I had been looking for a really long time trying to find tutorials on how to load motion capture data into Blender. And I wasn't finding it.

**[00:10:00]** And then I eventually found a tutorial series by a guy, Royal skies is his YouTube name. And he had a series about using a Kinect, like a Microsoft Kinect, get motion capture data. And the long and short of that tutorial series is that it was awful. Like it was a terrible experience. It was unreliable.

**[00:10:18]** Like you're using like this old broken software. It's unsupported by Microsoft. There's no way to really link more than one of them together if you don't want to pay someone thousand dollars for their software. And I realized that the reason why there was no available tutorials, free open source software using motion capture is that people don't really have access to motion capture equipment. Cheapest option out there.

**[00:10:40]** Cheapest options are like, you know, minimum, a few hundred if you want to do something like really don't want to use and like 5000 for like an IMU suit or like beyond that, it's like, you know, we're talking like five, six figures worth of equipment to get viable motion capture in that space. I was sort of thinking on the list of gross inequities of the world, like inaccessibility of motion capture is like pretty low down. But you know, it is on the list like. And so it was, it became a thing that I could feel like I could focus on. It's like, okay, well I'm going to provide this as like, you know, until that point I was thinking like, this is a tool that I will use for my research.

**[00:11:15]** And then that was first point of thinking that this is something that actually would be wanted and desired by the world at large and could actually sort of fill a need that existed. And so that was in summer of 2020. And then so we worked sort of in that expectation that we're trying to make, you know, a motion capture software for someone. And then in summer of 2021, I made a Twitter post that, you know, showing me like juggling on a B and getting tracked by Open Pose and Deep Lab Cut, falling back on a history of posting viral videos to the Internet. And it had got a lot of attention, got a lot, and people really liked it.

**[00:11:51]** And so since then, the past like whatever, six months, there's been this kind of growing Realization, you know, for me and people in my lab and whatnot, that, you know, this is something that people actually want. This is something that what we're doing is, you know, on par with what is out there. Like, you know, there's really even like the paid versions of things are like, really not that good, and they're really not doing anything differently than what we're doing. It's just a little more polished. It's kind of been this sort of growing realization that, you know, like, you know, incrementally sort of putting more and more effort towards this thing and sort of incrementally realizing that the potential future of it is bigger and bigger, which is, you know, exciting, intimidating, terrifying, you know, mix of all those sort of fun feelings.

**[00:12:34]** That was also honestly, an awesome journey and story through that experience. It's so cool you, like, mention your philosophy and sort of introspection and like, I love how you reflecting on both your place in the world and how you contribute to it and putting your work in the context of everything that's going on is just awesome. The pandemic did a number on me for sure. This would have always been like a strange time for me, like starting a professorship, moving to a new place. But I don't think that those introspective moments would have had quite the same impact if I had not been a professor, like, if I'd still been a postdoc.

**[00:13:09]** Because when you're in academia, you're on temporary positions for pretty much the entire time, like undergrad, grad school, postdoc. Those are all temporary positions. Then you become a professor and all of a sudden you're part of the team, you're part of the institution. And when you're in academia, you have privilege, but you don't really have much power until you become a professor. And so that was sitting in that sort of feeling of like, oh shit, I have, like, I have power.

**[00:13:38]** I was already kind of feeling weird about that, like walking into it and then all of a sudden it's like everything else starts going down. I was just like, okay. No, but really though, like, what am I going to do? With great power comes great responsibility. Like, I love the mcu, Spider man, but like, how are you going to drop Uncle Ben for real?

**[00:13:55]** I know, every time. So you talked a lot about the different open source softwares that are part of like Open Pose and Deep Lab Cups that are part of Free mocap. I'm wondering they you Open Pose is very known for being open. And what is the significance to Free mocap and why not open mocap or some other. Yeah, and maybe too I think you've talked a lot about the need for it, but maybe also the like overview of what it is.

**[00:14:22]** Exactly. I just am like wait, do we like really what is like the meat of freemo Cap? Why is it different from other macro less motion capture softwares that are available right now? Well the nice thing about freemo Cap is that it produces its own advertising. So if you just post this along with a video, everyone will know what it is before.

**[00:14:39]** So what it is, it's a free open source markerless motion capture system. So basically it's designed to work with effectively any cameras including like $20 webcams and GoPros. And we haven't used more expensive cameras yet because that was part of that shift that you know, around that sort of summer 2020. My plan at the time was use GoPros and then when that works, shift up to more expensive research grade cameras. In that sort of shift I was like actually let's see what the cheapest possible system and it turns out that the floor on that is $20.

**[00:15:09]** Like if you pay less than 20 dol webcam it doesn't work. I don't know why I don't understand cameras but, but it's effectively a way to use, you know, off the shelf hardware and open source software to achieve viable three dimensional. What I'm calling research grade motion capture data or research grade means that I would use it for my research and so I'm a scientist, therefore it counts. So that's the hope is, is basically the tagline is free motion capture for everyone. Like using the tools that are becoming available through the advance of technology and being made available by open source groups like openpose, Deep Lab Cut, anypose, any of these folks and basically trying to put them into a package that people can use.

**[00:15:52]** So back in 2017 I'm already a postdoc here, so I'm not untrained in anything but it was incredibly difficult to get openpose running on my computer. I saw it and I was like I need to use that, I need to figure out how to use it. So I went to their, you know, their GitHub and like I sort of tried to work through their instructions and it was months, it was an effort of months to even get it running. So there's kind of this tension right, because there's all of these people out there, amazing groups, you know, obviously big one Deep Lab Cut is another one that I, you know, sort of modeled My, you know, early stuff after and you know, a lot of groups that are making these open source technologies based off of like these absolute future shaping tech that's coming down the line. You know, machine learning, neural networks, convolutional neural networks, computer vision, what people sort of broadly call AI, but we don't use that term in this field because it's not specific enough.

**[00:16:45]** But there's all this amazing stuff that is available free and open source, but to use it you need to have this incredible. There's a really high tech barrier to being able to use it because people make the thing, they throw it up on GitHub and usually just call it a day. Which I now in retrospect, having worked on this for so long, I now know it's because making something usable by the general population is incredibly difficult. So right now Free mocap is basically just a duct tape job where I'm taking all of these great tools that are online that are available and free open source softwares and trying to duct tape them together into package that is sort of usable and user friendly and is accessible to people who don't have that kind of technical training and ideally could serve as an inroad for people to get that kind of technical training. Because if you're going to use the thing, if you design the tool correctly, using it teaches you how it works.

**[00:17:38]** That's kind of part of the aspirational aspects of the current design process. I like that philosophy of using it teaches you how it works. That's the hope, right? And it's interesting because you know, talking about free software, free open source software or whatever. So in contrast, if you are building a proprietary software, if your entire business model is I'm going to sell you this software that does markerless motion capture or whatever and I'm going to support myself by putting a very high, you know, five figure ish price tag on it.

**[00:18:08]** You actually have a vested interest in ensuring that the people who are using your software never actually understands how it works. All of the core equipment, all the core software, biomechanics is just linear algebra. All these things are just massive GUIs to solve linear algebra equations. And yet they cost, you know, $5,000 for a sport license or $23,000 for a license. And you know, all of these things can be done, you know, in Python or in Matlab.

**[00:18:39]** Matlab's also not open source, but Octave. I think that that's one of the great advantages of being a free open source software is that I can commit to the design that whoever uses the software for long enough will learn how it works. You know, I want you to understand every little bit and piece of it. But if I was trying to sell this thing as like a secret black box that does the thing that only I can do for you, I have a vested interest in the opposite. I don't want you to understand.

**[00:19:04]** I want you to know how to use my tool, but I don't want you to know enough to be able to recreate it on your own. Yeah, it's so interesting because I feel like a lot of the marker list motion capture systems, as you're saying, are quite challenging to set up and require an extensive background of coding as well. And it's hard because a lot of these are developed by scientists who do want to help and do want to make an impact and, and would love to see their project move forward into something that's usable. But I think there are some barriers to that in that Sometimes these are PhD projects and then a person graduates and it doesn't continue. Is there funding to support translating this tool into, you know,

---

### Chunk 3 [00:19:30 - 00:29:30]

**[00:19:30]** Move forward into something that's usable. But I think there are some barriers to that is in that Sometimes these are PhD projects and then a person graduates and it doesn't continue. Is there funding to support translating this tool into beyond just you publish the paper and then you upload the code to GitHub. But it's like that's not really necessarily. Then someone's going to be able to download the code and put it together.

**[00:19:56]** It's so much more complicated than that. And having some really usable and simple tool to be able to do markerless motion capture, I could see could be really powerful. And I'm seeing that in one of my studies now is we developed a tool to do that just like a web app where people can upload videos and it automatically uses open pose to process it and gets results from that. We've had so many people reach out and ask if they can use it. When you get down to it, we're really just running open, calculating joint angles.

**[00:20:28]** But it's made me realize how hard that is for people and it's hard for me too to figure that out as well. So. And also with, you know, where do you process these? You need storage space, you need security. It's like, it's a lot like, how do you manage all of this?

**[00:20:44]** And to me, it seems like kind of an overwhelming. You hit a couple really important things. I think there. One is that I'll talk about the second thing you said first. The structure of academia doesn't support these kinds of activities.

**[00:20:58]** We operate on the basis of students who are supposed to be, you know, they're temporary, they're not allowed to stick around, they're supposed to move on and try to, you know, be another one of you, even though there's not enough spots in the world to do that in papers they get, you know, you publish them and then you walk away. And we really don't get rewarded for things like going back and forth, going back. And like, that's not, you know, the gatekeeping dinosaurs above us on the hierarchy aren't impressed by that because dinosaurs are never impressed by the future dinosaurs. So there's a problem of, like, the activities that are necessary to make things usable are not rewarded. In addition, making things usable is really hard.

**[00:21:39]** It's very difficult to make something that anyone can use. And so there's a term in the software world of academic spaghetti code, which is the way that we tend to code discovery mode thing where like, you start out, like, I don't know if you can make this thing possible and you do the complicated thing and it does the thing and then don't touch it. And that's how we do. And that's the scientific process, that's discovery. But to make the thing actually usable, it's a much longer and more challenging iterative process of, you know, you take it, you know, you thank it for what it taught you, put it in the ground and you start over again.

**[00:22:15]** You talk to people who work in the tech industry say hey, what is best coding practices? You actually spend the time doing like the soft skills of, I'm doing air quotes for listening of like user interface design, user experience design. Like yes, we in the sciences, we don't respect it because it's not fancy enough and like it's a challenge. So I don't know. The question is how do you do it?

**[00:22:38]** The answer is I don't. I've never known what to do. So it's mostly asking for help. You know, the only way that I've managed to sort of take my weird long meandering path is to just kind of like look for the people around me that seem like they're doing the things that I want to do and then just copy them. So groups like Deep Lab Cut were really, you know, they're like one of the few people.

**[00:22:58]** The technical capacity of Deep labcat is impressive but I think the reason why they've had the impact that they have is that they've built into year round it. I'll just say that GUI has a special place in my heart but they did the effort to make it accessible to people that can't run code from the command line and can't write their own Python scripts. And so because of that people use it. I saw them kind of trying to build that bridge between like high level machine learning, computer science and like biology, neuroscience kind of people and sort of thinking that I kind of want to start do that, build that same kind of bridge but instead of building it from like scientist to scientist, build it from like scientists to like general population, which is easier to do because I'm not actually making the tools. I don't know how like I don't know how to do like the machine learning, neural network stuff that it underlies these technologies.

**[00:23:44]** Like I kind of know how it works, but I can't make it myself. I know how to use it. So that becomes the whole job is like how do you wrap these things in a package that is actually accessible to people that are not high level computer scientists, kind of the like Netflix of Like just hosting, maybe not Netflix, but it's duct tape. I mean it's basically saying it's like, okay, here's a cool thing and here's a cool thing and I'm going to duct tape it together so that you know, you can use it as a whole as opposed to like, you know, having to use the pieces in their sort of, in the form that they sort of are presented like you. Because people present their stuff as usable as they can get it.

**[00:24:19]** And so then if I can sort of, you know, me and my lab, if we can go in there and figure out how to use it from that perspective and then just like smooth that out, like build a user interface for it and make it so that you don't have to, you can interface with it in a way that we recognize and like GUI's and button clicks and whatnot. That's pretty good. The other question you're asking about like hosting and privacy and longer term stuff, we will when we get there. I have friends who are sort of working with us now who are decade plus experience in like tech startup software development. So they know how that works.

**[00:24:50]** So I'll ask them when it comes up. So you're not creating or training a new model, you're packaging these other softwares in a way that's accessible. How do you imagine as there's multiple different markerless motion capture softwares popping up, they're improving and coming out with new versions, there's going to be a, you know, upkeep needed to keep that working. And I'm curious, I guess, for that in general, but then also as a free and open source software, how do you continue that upkeep and making it free to people. So this is very much a long term project for me.

**[00:25:31]** This is not a hit and run, drop it on GitHub and call it a day thing. I am thinking and planning. You know, I formed the Free Mocap foundation nonprofit. We're trying to get 501C3 status which is approved by the university and you know, software disclosures and, and all that. But you know, trying to think about the questions of like, how do you maintain long term viability and like how do you keep afloat?

**[00:25:52]** Especially considering what we just said about how like the standard trappings of academic science really don't support this kind of endeavor. One of the questions you asked is easy about the different technologies and the different tools that are coming out. And that's actually always been sort of a core part of the project is like, we want it to Work for all of those. Like we started with Open Post. You've been using media.

**[00:26:09]** We fished a mediapipe because it's a little easier to use. Not quite as accurate, but much faster and easier to use. E Blab gets in there, sort of like have like a train, your own network and there's a bunch of other options that people keep sending to me and I haven't had time to integrate them yet. But you know, the software is designed to sort of really focus on that sort of transition step I mentioned of like, okay, I want to see how your software works and I'm just going to reshape it into the way that it needs to be to work with free mocap thing. So that way, ideally as new things come out, then it's just you write the wrapper, you buy a little interface wrapper that says, okay, I'm going to take, you know, all of them taken video and spit out pixel locations.

**[00:26:46]** Some of them try to guess 3D from one camera, but it's not very good. But they are coming out, they are getting better and like who knows what the next big one is going to be tomorrow. And so the hope is to sort of have something that can interface with all of them and sort of provide that way. The other question of as a free software, how do you maintain viability? How do you get funding?

**[00:27:07]** How do you get money? That is a challenging question. And to that I have done the thing that I like to do, which is look around at other people who are doing it and see what they're doing. There are plenty of large scale open source projects, Blender, I've mentioned Python, Linux, you know, whatever, you know, gimp Free, cad, Inkscape, There's a ton of these out there. Alice Vision.

**[00:27:30]** So I basically stalk all of them. If most of them went, you know, numpy, matplotlib, like all these guys, I basically stalk their tax records, stock their websites, allow them to do annual reports. If they're a nonprofit, their taxes are public. Fun fact, any nonprofit, just Google name of the nonprofit IRS 990 and you'll see what their money, how they get their money and where it goes. Also works if you work for a private university.

**[00:27:54]** And it turns out what most of them do is a mishmash of sources. You get some large scale grants, small scale grants to sell your services. You know, maybe you have like maybe someday we start making our own cameras and we sell those. And so it winds up just being kind of like a patchwork network. And also like I still plan to use this to do my research.

**[00:28:12]** And my research is funded by the nih. So as long as they're like, hey, we like what you're doing. That was part of the kind of the thought process of this. It's like I already live, you know, as a professor, research professor, I'm already expected to get enough funding to support a research program. And as I'm sort of looking at this, it's like, you know, there's costs that are associated with this that are not associated with like standard academic research.

**[00:28:37]** But like I'm already kind of trained on like how to get money. It's just a matter of like kind of shifting the flavor a little bit and sort of trying to continue to build a free open source software. I just want to go back to one thing you said in there about filtering like, or about, you know, all these different people are coming to you and like they're excited about this, which is awesome, and excited to have their software integrated with yours. I guess I'm wondering what is the filtering process like and do you feel that's your responsibility for, you know, what are your thresholds for what's good enough to include? And also about, you know, how do you be mindful about being inclusive?

**[00:29:10]** Probably want to incorporate all of these different, you know, pose detectors that can do anything from, or sorry I should say video analyses that can do anything from animals to humans to xyz. Yeah. How are you doing that and what are maybe some of the challenges there too? So there's a lot of challenges. First of all, I just going to also just like for context with.

---

### Chunk 4 [00:29:15 - 00:39:08]

**[00:29:15]** Do anything from or sorry I should say video analyses that can do anything from animals to humans to xyz. Yeah. How are you doing that and what are maybe some of the challenges there too? So there's a lot of challenges. First of all I just going to also just like for context of like the dream of freemocat being like a, like an easy to use thing is aspirational.

**[00:29:35]** We are not there yet. We're working in that direction and we'll get there sooner rather than later. But that is kind of the goal. And so in terms of that level of inclusivity that's kind of. It's a continuous process.

**[00:29:45]** That's right. Vetting potential algorithms that get included. There's a really nice sort of filter built in right now which is like I really don't have the time to do that for most. So you know, I think that there's a futures where there's more time and more, more effort, more people working on this. But for now you know we're focusing on just like core capacity.

**[00:30:04]** And for that we're currently using any pose for the camera calibration. Open pose is supported but we also using MediaPipe now which is a Google project, which is cleaner, easier code and then Deep Lab Cut is sort of under supported at this point because they haven't focused on it recently. But it's kind of, it's a build your own, you can train your own network so you can kind of track whatever you want. So the questions you asked will come up eventually. But for now we're kind of focusing on like the core capacity specifically now is like 3D motion, 3D full body, hands and face of a human.

**[00:30:33]** Accurate enough that like I as a researcher will look at it and think that's something I could do my science with. But you're not wrong. I mean there's a lot of issues of inequity here. Like you know any neural network based is on what it was and if it's a camera then it can see things like your skin tone. So you must be really mindful of these.

**[00:30:53]** One of the nice things about Media Pipe is because it's a large scale Google project, they have you know, a section of the website about like you know, skin tone and things like that. It's a legitimate concern. Good that people are looking into it. There's a documentary called Coded Bias on Netflix which is nice but also like at this point in development like, like I'm not really in a place to be like changing the underlying algorithm. So there's a there's a little bit of trust going on there that the people who are making them are kind of doing their best.

**[00:31:21]** Speaking of trust, I can't say trust and Google in the same word. That's not a good idea. Like, I am absolutely concerned about like the thing. So here's the thing about markerless motion capture. It's going to happen somehow, somewhere.

**[00:31:37]** Like we have the ability to track people accurately through cameras and you better believe we're going to do it as a culture. You know, technology advances autonomously and I absolutely am concerned about what free mocap would look like if it was Facebook mocap. You know, like there are massive privacy concerns there. All I can really hope is that, you know, being making a free open source version of this thing, which is absolutely going to be a technology that is a part of our future, hopefully that will provide some sort of protection from a potential world where that technology developed by one of these mega tech corporations who have proven time and time again that they cannot be trusted. I appreciate your points about the inequities sometimes in machine learning models.

**[00:32:27]** And I've seen that in our study as well. We see women who are wearing long skirts that it's having a hard time with that depending on people's weight, it is less accurate. And yeah, it's frustrating to see too, as a researcher, but then knowing that we've only been training on this certain types of data and now if you are able to create the software or a GUI or a way that we can more easily capture video and do markerless motion capture, then perhaps we can also start collecting more data. And I would hope that maybe those companies would then want to incorporate those and retrain their models and do things that are more inclusive and we continue to improve in that way. So I appreciate the freemo cap will I think contribute to that potentially as well.

**[00:33:18]** I mean, one of the things that I found really is helpful, but the only thing that's really helpful is when you're interacting with a new discipline that you don't know much about, to learn the culture of that, of those researchers, what they value, you know, how they choose their questions, what they consider an explanation, what does their data look like. And so as I interact with the computer science community, the research side of computer science that are actually building these algorithms, the corporations aren't building the algorithm. These are being done by labs and then bought by corporations. That community is driven by benchmarks. Either you do something like an open pose where you come out like, hey, I'm doing something Literally no one else can do.

**[00:33:53]** You know, most people don't do that. So what they do is they, they compete on benchmarks. And so you say, oh Well I got 94, you know, you got 94% on the Coco data set and I got 95.6. And therefore I like literal like leaderboards and say like, you know, there's in bold and like that's how that community considers progress. So when you have that kind of methodology for, for establishing who works on what, it has these huge blind spots because as a community to computer science, they move really fast and they have no attention span.

**[00:34:24]** So like a lot of them kind of are acting like two dimensional tracking is a solved problem because it's a little like as quote someone, like it's a little too close to the pixels. You know, they're always trying to scale up. Now they want 3D, now they want volumetric tracking, now they want faster, they want us to run on your phone. And they're not trying to make it more accurate, they're not trying to make it more inclusive. They're not trying, you know, because.

**[00:34:50]** And I think the reason why is because they don't have, it's not obvious to them that that's a need. So I'm hope one of them in the myriad of hopes around this project is that if, you know, we can come up with definitive methods to say, hey, look like on these metrics that matter, here's where you all stack up. If there can be a quantitative measurement of those kinds of things, here's hoping that the people who actually have the capacity to drive that technology forward will start working on it or I'll get the funding to hire the students. Thank you so much for sharing all that. This has been really interesting.

**[00:35:25]** We're really excited about the future of markerless motion capture and the ideas that you're posing and we hope we do see we're rooting for Free MOCAP in making some these amazing technologies more usable. And having a team behind that is really exciting to hopefully push that forward. Where that Blender, which is a free and open source computer graphics software, has been a big inspiration behind Freemo Cap. And we've heard you bring up Blender a few times as you've been talking about us and the story behind Freemo Cap. Can you share a little bit more about that inspiration?

**[00:36:00]** And then this connection between human movement and 3D animation, which is Blender. So Blender has been, you know, one of the people I like to sort of pretend copy. I think that, you know, My interest in them comes from kind of twofold. One is like what Blender is and what it can do. So basically like if you look at like an animation, you look at like, you know, Toy Story or whatever, you know, 3D animated, soft animated movie, you want to look at.

**[00:36:25]** What you're looking at is artists who have used a GUI to have an incredibly fine grained control over minute aspects of 3D space, which is an incredibly powerful that. You know, I've been trying to do that from like code and sort of raw level linear algebra for years. And it's knowing things at that level is very valuable, but it's limited. There's only so much you can do. So I think that having the tools that are available to animators, available to scientists, will make scientists better scientists and will also connect that artistic side of technology and 3D animation to the sciences and biomechanics that are happening.

**[00:37:04]** Other thing that's inspiring about Blender is that it is a great example of a large scale, free, open source software that has its own legs and is growing in really interesting ways. I look at that and I think of that sort of large scale collaborative endeavor with an actual end goal project that is usable and useful for general population. And I see that as like, that's a better model for science. That is how we should be doing science. I think the open source community is what science and academia pretends to be like.

**[00:37:38]** We are actually very siloed. We don't really work well together. We don't work on large scale collaborative projects. We do papers and piecemeal stuff because we're all fighting for survival in a world. So Blender has been a particular inspiration, both because of the capacity that it affords and also for what it represents.

**[00:37:55]** And again, it's been so nice to hear that echoed throughout this conversation. Like not only the awesome technology and accessibility, you know, things you're doing to make it more accessible, but also the thoughtfulness and intentionality that you have behind it. And I think that is a little bit on that artistic and creative side, like in your philosophy. And like, I don't know, it's just been really awesome to kind of go in and out of all those different perspectives in this conversation. So we've really enjoyed it and enjoyed talking with you.

**[00:38:24]** If people want to learn more about your work or more about Freemo Cap, how can they do that? So we have a lot of ways. Freemocap.org is the website, Twitter is the thing. So twitter.com freemo cap is where I do most of my, like, communication there. There's also Freemo Cap accounts on Twitch.

**[00:38:42]** There's a Discord server that you can get to through. Through the Twitter, the website. I don't know, you find all these things. So the website, we got Twitter, we got Twitch. There's technically a TikTok.

**[00:38:51]** I don't use it very much. There's not Facebook page and Instagram and I think there's a subreddit. If you want to send us those links, we'll add them all in the description so people can find them. Well, thank you so much, Jonathan. This is amazing and a great way to start our day.

**[00:39:06]** Well, thank you so much for the conversation. A lot of fun.

---
