# Transcript: DW22_Mattis, Jonathan S. - June 16th 2022, 12_33_15 pm [1_ucmxydwq]

## Source Information

- **Source Type:** Local File
- **File Path:** `C:\Users\jonma\Videos\DW22_Mattis, Jonathan S. - June 16th 2022, 12_33_15 pm [1_ucmxydwq].mp4`

---

**Total Duration:** 00:19:39

---

## Full Transcript

### Chunk 1 [00:00:02 - 00:10:00]

**[00:00:02]** Cool. All right. Hello, everybody. So I started my professorship in the summer of 2019, and let me tell you, things did not go as planned, as you might have expected from the research I've done before. My plan was to show up in Boston and continue the kind of laser skeleton, visual control of foot placement stuff that I've kind of been doing for my whole career.

**[00:00:31]** And eventually we did in fact, get to that. That's the stuff that Trent was presenting earlier in the week, just sort of walking overseas. And it's going quite well. Trent, by the way, is going on the job market this year. So if you want someone with that kind of skill set, he's somewhere.

**[00:00:51]** His ultimate dream is to be a research professor at a state school in the Midwest. So. So if you know anybody, he would do well in a sort of psychology, neuroscience type of environment. Anyway, so what became. So the plan was to study laser skeletons, visual control of foot placement, sort of that classic kind of stuff.

**[00:01:13]** And all that kind of went out the window when like, you know, like, the situation started to unfold. And so what became the free mocap project started as a stopgap. It was just sort of something that we could work on as a group while we were waiting to get access back to the lab. And it quickly kind of took over the lab as sort of started being thing to do, which also happened to coincide sort of right around when we were starting to get like, viability of the project. Also happened to coincide with the start of the George Floyd protests, at which point I think we all kind of started to ask these kind of questions about whether or not our position.

**[00:01:55]** We were sort of really adequately using our positions of power to do the most good for the most people. And I kind of generally had some beef with my particular institution, but also just the whole institution of higher learning and education in general was starting to kind of concern me. Sort of the idea of this sequestration of science and sort of like gatekeeping of knowledge. Not sort of, not like nefariously. It's just kind of how the structure of things worked.

**[00:02:23]** And so let's see, what are we doing here?

**[00:02:29]** And we got them there. Missing one. Missing you.

**[00:02:38]** Because of that.

**[00:02:43]** And so we started asking kind of the questions of like, oh, no one should ever do a live demo. Just to be clear, it's a bad idea. But anyone who knows me knows that I really like to make my life as difficult as possible.

**[00:03:09]** Also good to have backup plans if necessary.

**[00:03:16]** A not you. Cool.

**[00:03:23]** Pretty sure the laptop camera's camera one.

**[00:03:28]** We'll find out shortly I was wrong. It is camera three.

**[00:03:37]** And that one has its eyes closed.

**[00:03:42]** All right, gotta restart.

**[00:03:48]** I've never done this before in my entire life.

**[00:04:04]** And.

**[00:04:09]** And so basically I began to ask the questions. Okay, camera zero is rotated.

**[00:04:17]** That one is upside down. No, that one's good. And that one. Oh, that's camera zero. Can solve that problem mechanically.

**[00:04:30]** Hello.

**[00:04:40]** So the idea became to try to figure out, as we're sort of getting things starting to work with the motion capture system, and things started working more or less, the question became, what is the cheapest motion capture system that someone can make that can produce research quality data? Where research quality here is defined as stuff that I personally would use for my research. Camera zero is rotated. There we go. And that's also a little dark.

**[00:05:18]** There you go. Nope. Numloc7 go a and so with that sort of idea of trying to find a motion capture system that's as cheap as possible, the most free mocap project was born. Because on the list of global inequities, inaccessibility of motion capture systems, it's probably not particularly high, but it is on the list. And so around that sort of moments of sort of introspection and things like that.

**[00:05:56]** What am I doing there? See, that's too bright.

**[00:06:04]** I encountered a very powerful concept of lift where you stand. And the idea is that when you encounter a problem in inequity that sort of causes you some sort of moral harm, that you don't just turn around and drop everything that you've done in the hopes of doing sort of like going off to join Greenpeace or something like that. I don't know shit about the climate. I know about how to make laser skeletons. And so the concept of lift where you stand is when you're having a problem, what you do is you look around where you're standing, you find something that looks like a handle, you grab it and you pull as hard as you can in what feels like the right direction.

**[00:06:41]** Because on the list of global inequities, maybe motion capture is not particularly high, but what is high is inaccessibility of science and technology. There are all sorts of insane DW live no, not plus live nonsense.

**[00:07:02]** Because as we have all sorts of insane new technologies coming down the pipe, sort of all sorts of markless tracking methods, machine learning, AI. The people who really stand to benefit from these tools often can't access them. They don't have access, they don't understand them. Even the ones that are free and online 8 are the people who try to make them freely accessible. We have about like an 80% chance to success here.

**[00:07:31]** So people don't have the technical ability to use them. And even the people who do like people in this room who wants to spend the time sort of wrapping these things together into something that you can actually use to generate real behavior, real usable data. Okay. And so we sort of started doing this thing of trying to try and do the science, make a tool that's usable for me so I can enact my research while also remaining accessible to people without technical training and sort of sharing it out loud, sort of. Not only to the lovely people in this room.

**[00:08:07]** It's great to see you all again. But also to people outside. And so in the process, as we're sort of chugging along here, if it's going to fail, it's going to fail right here.

**[00:08:25]** We started building all of this sort of online infrastructure. I think we're good. Ish. We'll see. The skeleton might be a monster, but monsters can be friends.

**[00:08:37]** So anyone who's been sort of on Twitter has probably seen me sort of posting my ridiculous dancing gifs. This is the Free Mocap website. It kind of currently lives in this state of being as friendly of a GitHub repository as I can make it. But obviously there's room to grow. As you've probably noticed that this is a fully offline process.

**[00:09:00]** You probably would like for it to be real time. I would like for it to be real time, but I'm not that good of a programmer. But you know who is is my friend, Endurance Adehan. He has 16 years experience as a software architect for tech companies, and he's currently just accepted the position as CTO of the Free mocap Foundation, which is a nonprofit that I made. He gets no money, but he gets a nice title.

**[00:09:24]** So you can join. You can sort of follow. Here I am. It's on Twitter. I do Twitch streaming every now and again.

**[00:09:31]** I forgot your real name, but your username is Glitch Robot. Happy to see you in person. Here's the GitHub repository and here are the instructions. They're sort of as simple as I can make it. This is sort of a good time to remind people that my first degree is is a bachelor's degree in philosophy.

**[00:09:49]** So trust me, I have sympathy for people who have a lack of technical background. In the next coming months, Endurance and I will be restructuring the software.

---

### Chunk 2 [00:09:45 - 00:19:39]

**[00:09:45]** Remind people that my first degree is a bachelor's degree in philosophy. So trust me, I have sympathy for people who have a lack of technical background. In the next coming months, Endurance and I will be restructuring the software into what we're calling the Alpha release. It's currently version like 0, 0.52, 0.1 and above are going to be a completely different story that will have some form of real time reconstruction. And what fun that will be.

**[00:10:14]** We also have a Discord server. No, you don't. Those are not good friends. Has about 800 people on it. And one of the biggest communities that's shown excitement about this project have been independent video game makers, independent animators, people who basically are making their little projects in their basement and thought that something like motion capture was outside of their realm entirely.

**[00:10:38]** And I say, well, in fact, with nothing but three garbage webcams, $10 a pop, you can have your own decent motion capture system. And so here on this sort of free Mocap discussion, this is probably about a half a dozen to a dozen animators that sort of are communicating constantly about how you can reconstruct, how you can basically shape the data from free mocap into the kind of forms that they need to do their fun animation stuff. I'll show you some clips there a little bit later. Here's one. Here's Roald Dahl, who's made sort of a package in R to analyze the data.

**[00:11:19]** And then here's Manitoba Mike talking about analyzing the data of him and his son doing hockey slap shots. I'll show some of that in a second.

**[00:11:31]** And so this whole process of sort of trying to share the research as much as possible has been a lot of fun and is starting to now finally get to the point where it's producing data that I can look at and think, you know, maybe I can do something with that. So no promises about this particular one because there is like a really sensitive part in the middle there that may or may not have worked. So this might be a monster, but if we're lucky. Okay, grab in the Y. This is Blender, another free open source software.

**[00:12:07]** Rotate around X, grab on Z, click on U. Get closer and rotate around. Show me that material. Preview, please. And there you go.

**[00:12:25]** So that is a $30 motion capture system recorded in 11 minutes or less.

**[00:12:39]** Okay, I kind of want to pass out right now, but I think I'm going to continue the talk. I forgot to put my other talk, honestly. I mean, I brought all the cameras, but of course, that's a fun skeleton. But you Know, I promise you laser skeletons. And in fact I have laser skeletons.

**[00:13:03]** So this is a recording that I got from my living room. It is. This is a skeleton with lasers coming out of its face, built with a bunch of voltages recorded on the back of some silicon wafers and some garbage webcams and, and a pretty nice people labs eye tracker and then just a crap ton of linear algebra. The video's not synced and that doesn't bother me at all.

**[00:13:28]** So this is the kind of stuff that it's similar to the kind of data that you would want to do, the kind of research that I would normally do. The accuracy is not nearly as good as a traditional motion capture system, but the cost is roughly five orders of magnitude less. So I can accept a little bit of noise in that regard. I forgot to start my timer. How are we?

**[00:13:50]** What's that? Okay, cool. That is less than I would have hoped. You said. So I'm going to be doing a demo and my students are going to be doing some presentations after this talking about the accuracy.

**[00:14:03]** But this is the. These are the number of visitors to free mocap by country. In particular, I'm excited about that one, that's Nigeria, because it is the Nigerian film industry and I really want to see some free mocap data in Nigerians films. The GitHub is approaching 2000 stars. This is a small.

**[00:14:23]** This is a small number of people who have been presenting, making their own free mocap stuff. This is one of my favorites from user Vandalo. I don't know why we're not playing. Oh, because we're not actually presenting. There we go.

**[00:14:40]** So this is free mocap data with someone who knows how to make animation. And this is from user manitobamike who has been setting it up in his. Set up a motion capture system in his garage and has been recording his son doing stuff. This was his first one. It's okay, it has problems.

**[00:14:58]** So then on Twitch I did like a review of it and these have been his latter ones. These ones in particular are looking really good. And look at this kid, he's learning motion capture, he's learning computer vision, he's learning machine learning, he's learning kinematics and who knows what else. And this was sent to me out of the blue on Twitter. This is from a high school in Ohio.

**[00:15:23]** The students found it on Twitter and with no assistance from their teachers, set up a free mocaps area, calibrated it with the Churucco board and then brought all their teachers in and said hey, do A little dance for us.

**[00:15:40]** And I just think that's the coolest thing ever. That's their principle.

**[00:15:47]** So who knows, who knows where this will go? We're just getting started. We're just now getting to that point where the core processing pipeline is stable enough that it's probably start. It's worth starting to actually do stuff beyond there. This is a photogrammetry scan of my apartment.

**[00:16:02]** Don't worry about that.

**[00:16:06]** And here's my limerick. Today I've presented a roadmap for a project we're calling Free mocap. You can build skeletons as markerless friends. And I'm giving way webcams with gift wraps.

**[00:16:26]** So these are. This is 45 webcams, which by my math is approximately 15 motion capture systems. I will be giving them away to anyone that wants them and maybe some people that don't. So come find me in the whatever thing later and get a motion capture system.

**[00:16:55]** What would you like to have?

**[00:17:00]** So we've been intentionally pushing it as low into the hardware requirements as possible. Chris is going to be presenting on using GoPros, sort of like wide field of view, wide resolution, higher frame rate cameras. There's actually some additional challenges that pop up from that, but ultimately. So you could also incorporate LiDAR, you can incorporate Stereo Camera, RGBD. The calibration should work for those as well.

**[00:17:27]** But sort of the part of the divergence that happened because I was originally using GoPros and then the plan was once it works on GoPros scale up to your flir, your basils, your sort of higher research grade cameras. And then around the time of the George Float protest, I was like, well, what if we just pushed in the other direction and what if instead of sort of pushing it towards research grade stuff, I push it towards the most garbage cameras you can possibly get on the assumption that if it works for the cheap cameras, it will also work for the good cameras, but not necessarily the other way around. So now that it is stable and there is that sort of minimal cost option out there now is when I'm starting to sort of explore sort of higher grade options. It's hard to get better than GoPros for outdoor stuff, for indoor stuff. I think adding depth cameras, lidar, that kind of thing, higher frame rate cameras will really, I think be very beneficial for getting towards dynamics and things like that.

**[00:18:25]** If you're recording 240 frames per second, you can sacrifice half of them so that the remaining 120 frames per second will have higher reliability and things like that.

**[00:18:39]** Do you need a human shaped target. Great question. So, we've been using pre trained models for humans, but we've designed the software to be fairly agnostic to the tracking software. So if you sort of watch the end of Chris's talk, you'll see an implementation of a Deep Lab Cut model. So Deep Lab Cut is sort of like a train your own network usually made for.

**[00:19:00]** Oh, that's the other kind of. The other part. Just finished processing. And basically, if you have videos of, I don't know, like a robot, and you were to sort of click on the frame to sort of train the network, you basically slot that in and have something that, as long as you have something that can track the person or the object in pixel coordinates, it will feed into the rest of the reconstruction pipeline and everything else should play nicely.

**[00:19:38]** Yeah. So, next speaker.

---
